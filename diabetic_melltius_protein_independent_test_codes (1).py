# -*- coding: utf-8 -*-
"""Diabetic_Melltius_Protein_Independent_Test_codes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVn4g2pyLGC8vIz5ojVSVBMRMz1Mslzm
"""

from google.colab import drive
drive.mount('/content/gdrive')
import os
os.chdir('/content/gdrive/MyDrive')
from google.colab import drive
drive.mount('/content/drive')

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout
from keras.optimizers import SGD
from keras.initializers import random_uniform

dftrain = pd.read_csv('/content/DiabeticPositive1.csv')
#df.head()

##Label and ID represent dataset

Y=dftrain['class']
X=dftrain.drop(['class'],axis=1)

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')

# Option 1: Drop rows with NaN values
X = X.dropna()

# Option 2: Fill NaN values with a default value (e.g., 0)
# X = X.fillna(0)

# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))



# libaray For differnt measure
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.metrics import precision_score, matthews_corrcoef, recall_score, roc_auc_score
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, matthews_corrcoef, recall_score, roc_curve, auc
from sklearn.model_selection import train_test_split
##Split Data
from sklearn.metrics import accuracy_score
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten
from sklearn.manifold import TSNE
from sklearn.datasets import make_moons, make_circles, make_classification
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score, roc_curve, auc
import matplotlib.pyplot as plt

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Support Vector Machine": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Extra Trees": ExtraTreesClassifier(random_state=42)
}

# Initialize plot for ROC curves
plt.figure(figsize=(10, 8))

# Iterate through models, train, predict, and evaluate
results = {}
for name, model in models.items():
    print(f"Training and evaluating {name}...")
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    y_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Metrics
    accuracy = accuracy_score(Y_test, y_pred) * 100
    balanced_acc = balanced_accuracy_score(Y_test, y_pred) * 100
    f1 = f1_score(Y_test, y_pred, average='macro') * 100
    precision = precision_score(Y_test, y_pred, average='macro') * 100
    mcc = matthews_corrcoef(Y_test, y_pred) * 100
    roc_auc = roc_auc_score(Y_test, y_probs) * 100 if y_probs is not None else None

    results[name] = {
        "Accuracy": accuracy,
        "Balanced Accuracy": balanced_acc,
        "F1-Score": f1,
        "Precision": precision,
        "MCC": mcc,
        "ROC AUC": roc_auc
    }

    # Print results
    print(f"{name} - Accuracy: {accuracy:.3f}%, Balanced Accuracy: {balanced_acc:.3f}%, F1-Score: {f1:.2f}%, Precision: {precision:.2f}%, MCC: {mcc:.2f}%, ROC AUC: {roc_auc if roc_auc is not None else 'N/A'}")

    # Plot ROC Curve if applicable
    if y_probs is not None:
        fpr, tpr, _ = roc_curve(Y_test, y_probs)
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

# Finalize the ROC curve plot
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for All Models')
plt.legend(loc="lower right")
plt.show()



from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create SVC classifier with RBF kernel
svc = SVC(kernel='rbf')

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [0.1, 1, 10],

}

# Create GridSearchCV instance
grid_search = GridSearchCV(svc, param_grid, cv=5)

# Fit the grid search on the training data
grid_search.fit(X_train, Y_train)

# Get the best parameters from the grid search
best_C = grid_search.best_params_['C']
best_gamma = grid_search.best_params_['gamma']

# Create SVC classifier with best parameters
svc_best = SVC(kernel='rbf', C=best_C, gamma=best_gamma)

# Fit the classifier on the training data with the best parameters
svc_best.fit(X_train, Y_train)

# Predict on the test set
svc_pred = svc_best.predict(X_test)

# Evaluate on the test set
print("Accuracy on test set = ", accuracy_score(svc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(svc_pred, Y_test) * 100)

# Calculate additional metrics
c = confusion_matrix(Y_test, svc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)
f1 = f1_score(svc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(svc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, svc_pred) * 100
print("matthews_corrcoef", mcc)
roc_auc = roc_auc_score(Y_test, svc_pred) * 100
print("Area under the curve", roc_auc)
print('Confusion Matrix : ', c)

print("  ......      ")

# # Predict on the independent test set
# svc_pred_indep = svc_best.predict(dftrain)

# # Evaluate on the independent test set
# print("Accuracy on independent train set = ", accuracy_score(svc_pred_indep, dftrain) * 100)
# print("Balanced Accuracy = ", balanced_accuracy_score(svc_pred_indep, dftrain) * 100)

# # Calculate additional metrics
# c = confusion_matrix(dftrain, svc_pred_indep)
# sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
# print('Sensitivity : ', sensitivity1)
# specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
# print('Specificity : ', specificity1)
# f1 = f1_score(svc_pred_indep, dftrain, average='macro') * 100
# print("F1-Score : ", f1)
# precision = precision_score(svc_pred_indep, dftrain, average='macro') * 100
# print("Precision : ", precision)
# mcc = matthews_corrcoef(dftrain, svc_pred_indep) * 100
# print("matthews_corrcoef", mcc)
# roc_auc = roc_auc_score(dftrain, svc_pred_indep) * 100
# print("Area under the curve", roc_auc)
# print('Confusion Matrix : ', c)





# Get decision function scores for ROC Curve on independent test set
svc_scores_indep = svc_best.decision_function(X_test)  # Use X_test instead of Y_test

# Calculate ROC Curve for independent test set
fpr_indep, tpr_indep, _ = roc_curve(Y_test, svc_scores_indep)  # Use Y_test for true labels

# Calculate AUC for the ROC Curve
roc_auc_indep = auc(fpr_indep, tpr_indep)

# Plot ROC Curve for independent test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_indep, tpr_indep, color='darkorange', lw=2, label='ROC curve (SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Independent Test Set')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score

rfc = RandomForestClassifier()
rfc.fit(X_train, Y_train)
rfc_pred = rfc.predict(X_test)
print("Accuracy on testing = ", accuracy_score(rfc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(rfc_pred, Y_test) * 100)

# Calculate additional metrics
c = confusion_matrix(Y_test, rfc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)
f1 = f1_score(rfc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(rfc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, rfc_pred) * 100
print("matthews_corrcoef", mcc)
roc_auc = roc_auc_score(Y_test, rfc_pred) * 100
print("Area under the curve", roc_auc)
print('Confusion Matrix : ', c)

print("  ......      /")

# rfc_pred = rfc.predict(X_indep_test)
# print("Accuracy on indep testing set = ", accuracy_score(rfc_pred, Y_indep_test) * 100)
# print("Balanced Accuracy = ", balanced_accuracy_score(rfc_pred, Y_indep_test) * 100)

# # Calculate additional metrics
# c = confusion_matrix(Y_indep_test, rfc_pred)
# sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
# print('Sensitivity : ', sensitivity1)
# specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
# print('Specificity : ', specificity1)
# f1 = f1_score(rfc_pred, Y_indep_test, average='macro') * 100
# print("F1-Score : ", f1)
# precision = precision_score(rfc_pred, Y_indep_test, average='macro') * 100
# print("Precision : ", precision)
# mcc = matthews_corrcoef(Y_indep_test, rfc_pred) * 100
# print("matthews_corrcoef", mcc)
# roc_auc = roc_auc_score(Y_indep_test, rfc_pred) * 100
# print("Area under the curve", roc_auc)
# print('Confusion Matrix : ', c)

# Calculate probabilities for ROC Curve on test set
rfc_probs_test = rfc.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
fpr_test, tpr_test, _ = roc_curve(Y_test, rfc_probs_test)
roc_auc_test = roc_auc_score(Y_test, rfc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label='ROC curve (RF area = %0.3f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score

# Initialize and train the Extra Trees Classifier
etc = ExtraTreesClassifier()
etc.fit(X_train, Y_train)
# Make predictions on the test set
etc_pred = etc.predict(X_test)

# Calculate and print metrics
print("Accuracy on testing = ", accuracy_score(etc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(etc_pred, Y_test) * 100)

# Calculate confusion matrix
c = confusion_matrix(Y_test, etc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)

# Additional metrics
f1 = f1_score(etc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(etc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, etc_pred) * 100
print("Matthews Correlation Coefficient: ", mcc)
roc_auc = roc_auc_score(Y_test, etc_pred) * 100
print("Area Under the Curve: ", roc_auc)
print('Confusion Matrix: ', c)

# Calculate probabilities for ROC Curve on test set
etc_probs_test = etc.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
etc_fpr_test, etc_tpr_test, _ = roc_curve(Y_test, etc_probs_test)
etc_roc_auc_test = roc_auc_score(Y_test, etc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(etc_fpr_test, etc_tpr_test, color='darkorange', lw=2, label='Extra Tree ROC curve (area = %0.3f)' % etc_roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Early Stopping and Learning Rate Reduction Callbacks
earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

# Define CNN Model
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the Model
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
cnn_model.summary()

cnn_model.fit(X_train, Y_train, validation_split=0.2,batch_size=32, epochs=100, verbose=1)

# Evaluate on Test Set
predictions = cnn_model.predict(X_test).flatten()
y_pred = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred)
ba = balanced_accuracy_score(Y_test, y_pred)
cm = confusion_matrix(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
mcc = matthews_corrcoef(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- CNN Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Balanced Accuracy: {ba * 100:.3f}%")
print(f"F1-Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Matthews Correlation Coefficient: {mcc:.3f}")
print(f"Sensitivity (Recall): {recall * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"Confusion Matrix:\n{cm}")

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model=cnn_model.predict(X_test)
# calculate roc curves
cnn_model_probs = probs_cnn_model
cnn_model_probs = cnn_model_probs[:, 0]
cnn_model_auc = roc_auc_score(Y_test, cnn_model_probs)
cnn_model_fpr, cnn_model_tpr, threshold = roc_curve(Y_test, cnn_model_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model_fpr, cnn_model_tpr, color='Grey', lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model_auc)





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Early Stopping and Learning Rate Reduction Callbacks
earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

# Define CNN Model
cnn_model2 = Sequential([
    Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the Model
cnn_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
cnn_model2.summary()

cnn_model2.fit(X_train, Y_train, validation_split=0.2,batch_size=32, epochs=100, verbose=1)

# Evaluate on Test Set
predictions = cnn_model2.predict(X_test).flatten()
y_pred1 = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred1)
ba = balanced_accuracy_score(Y_test, y_pred1)
cm = confusion_matrix(Y_test, y_pred1)
f1 = f1_score(Y_test, y_pred1)
precision = precision_score(Y_test, y_pred1)
mcc = matthews_corrcoef(Y_test, y_pred1)
recall = recall_score(Y_test, y_pred1)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- CNN Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Balanced Accuracy: {ba * 100:.3f}%")
print(f"F1-Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Matthews Correlation Coefficient: {mcc:.3f}")
print(f"Sensitivity (Recall): {recall * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"Confusion Matrix:\n{cm}")

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model2=cnn_model2.predict(X_test)
# calculate roc curves
cnn_model2_probs = probs_cnn_model2
cnn_model2_probs = cnn_model2_probs[:, 0]
cnn_model2_auc = roc_auc_score(Y_test, cnn_model2_probs)
cnn_model2_fpr, cnn_model2_tpr, threshold = roc_curve(Y_test, cnn_model2_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, color='Grey', lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model2_auc)



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Define MLP Model
mlp_model = Sequential([
    Dense(128, activation='relu', input_shape=(153,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the Model
mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Summary
mlp_model.summary()

mlp_model.fit( X_train, Y_train, validation_split=0.2, batch_size=32,epochs=100,verbose=1)

# Evaluate on Test Set
predictions = mlp_model.predict(X_test).flatten()
y_pred = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred)
ba = balanced_accuracy_score(Y_test, y_pred)
cm = confusion_matrix(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
mcc = matthews_corrcoef(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- MLP Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Balanced Accuracy: {ba * 100:.2f}%")
print(f"F1-Score: {f1 * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Matthews Correlation Coefficient: {mcc:.2f}")
print(f"Sensitivity (Recall): {recall * 100:.2f}%")
print(f"Specificity: {specificity * 100:.2f}%")
print(f"Confusion Matrix:\n{cm}")

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1)
mlp_probs = mlp_model.predict(X_test).flatten()

# Calculate ROC AUC
mlp_auc = roc_auc_score(Y_test, mlp_probs)

# Compute ROC Curve
mlp_fpr, mlp_tpr, thresholds = roc_curve(Y_test, mlp_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(mlp_fpr, mlp_tpr, color='blue', lw=2, label='ROC curve (MLP AUC = %0.3f)' % mlp_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for MLP model: %.3f" % mlp_auc)



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Define the LSTM model
lstm_model = Sequential([
    LSTM(128, activation='tanh', return_sequences=True, input_shape=(153, 1)),
    Dropout(0.2),
    LSTM(64, activation='tanh'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # For binary classification
])

# Compile the model
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
lstm_model.summary()

lstm_model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)

# Make predictions on the test set
lstm_predictions = lstm_model.predict(X_test)
lstm_pred_classes = (lstm_predictions > 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(Y_test, lstm_pred_classes)
precision = precision_score(Y_test, lstm_pred_classes)
recall = recall_score(Y_test, lstm_pred_classes)
f1 = f1_score(Y_test, lstm_pred_classes)
roc_auc = roc_auc_score(Y_test, lstm_predictions)

# Confusion Matrix
cm = confusion_matrix(Y_test, lstm_pred_classes)

# Print evaluation metrics
print("LSTM Model Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1) from the LSTM model
lstm_probs = lstm_model.predict(X_test).flatten()

# Calculate ROC AUC
lstm_auc = roc_auc_score(Y_test, lstm_probs)

# Compute ROC Curve
lstm_fpr, lstm_tpr, thresholds = roc_curve(Y_test, lstm_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(lstm_fpr, lstm_tpr, color='blue', lw=2, label='ROC curve (LSTM AUC = %0.3f)' % lstm_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - LSTM')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for LSTM model: %.3f" % lstm_auc)



# Save the trained model
lstm_model.save('lstm_model.h5')
print("Model saved as 'lstm_model.h5'")

from tensorflow.keras.models import load_model

# Load the saved model
loaded_lstm_model = load_model('lstm_model.h5')
print("Model loaded successfully.")

# Display the model summary to confirm
loaded_lstm_model.summary()

# Reshape test data (if not already reshaped)
X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Make predictions
lstm_predictions = loaded_lstm_model.predict(X_test_lstm)
lstm_pred_classes = (lstm_predictions > 0.5).astype(int)

# Evaluate the loaded model
accuracy = accuracy_score(Y_test, lstm_pred_classes)
print(f"Accuracy of the loaded model: {accuracy:.4f}")



from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Save model to Google Drive
lstm_model.save('/content/drive/My Drive/lstm_model.h5')
print("Model saved to Google Drive.")

# Load the model from Google Drive
loaded_lstm_model = load_model('/content/drive/My Drive/lstm_model.h5')
print("Model loaded from Google Drive.")



from sklearn.metrics import roc_curve, roc_auc_score


plt.figure(figsize=(9,8))

plt.plot(fpr_indep, tpr_indep, lw=2, label='ROC curve (SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot(fpr_test, tpr_test, lw=2, label='ROC curve (RF area = %0.3f)' % roc_auc_test)
plt.plot(etc_fpr_test, etc_tpr_test, lw=2, label='ROC curve (ET area = %0.3f)' % etc_roc_auc_test)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot(mlp_fpr, mlp_tpr, lw=2, label='ROC curve (MLP AUC = %0.3f)' % mlp_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='ROC curve (LSTM AUC = %0.3f)' % lstm_auc)

plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)
#plt.plot([0, 1], [0, 1],  lw=2, color="r", label="Amino Acid Composition ROC Curve", alpha=0.8)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()