# -*- coding: utf-8 -*-
"""Diabetic_Melltius_Protein_Independent_Test_codes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Qd-G7jsyCHdwJC2g-FuqoLuxlyUPUZQ
"""

from google.colab import drive
drive.mount('/content/gdrive')
import os
os.chdir('/content/gdrive/MyDrive')
from google.colab import drive
drive.mount('/content/drive')

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout
from keras.optimizers import SGD
from keras.initializers import random_uniform

dftrain = pd.read_csv('/content/DiabeticPositive1.csv')
#df.head()

##Label and ID represent dataset

Y=dftrain['class']
X=dftrain.drop(['class'],axis=1)

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')

# Option 1: Drop rows with NaN values
X = X.dropna()

# Option 2: Fill NaN values with a default value (e.g., 0)
# X = X.fillna(0)

# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))



# libaray For differnt measure
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.metrics import precision_score, matthews_corrcoef, recall_score, roc_auc_score
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, matthews_corrcoef, recall_score, roc_curve, auc
from sklearn.model_selection import train_test_split
##Split Data
from sklearn.metrics import accuracy_score
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten
from sklearn.manifold import TSNE
from sklearn.datasets import make_moons, make_circles, make_classification
import matplotlib.pyplot as plt



from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create SVC classifier with RBF kernel
svc = SVC(kernel='rbf')

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [0.1, 1, 10],

}

# Create GridSearchCV instance
grid_search = GridSearchCV(svc, param_grid, cv=5)

# Fit the grid search on the training data
grid_search.fit(X_train, Y_train)

# Get the best parameters from the grid search
best_C = grid_search.best_params_['C']
best_gamma = grid_search.best_params_['gamma']

# Create SVC classifier with best parameters
svc_best = SVC(kernel='rbf', C=best_C, gamma=best_gamma)

# Fit the classifier on the training data with the best parameters
svc_best.fit(X_train, Y_train)

# Predict on the test set
svc_pred = svc_best.predict(X_test)

# Evaluate on the test set
print("Accuracy on test set = ", accuracy_score(svc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(svc_pred, Y_test) * 100)

# Calculate additional metrics
c = confusion_matrix(Y_test, svc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)
f1 = f1_score(svc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(svc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, svc_pred) * 100
print("matthews_corrcoef", mcc)
roc_auc = roc_auc_score(Y_test, svc_pred) * 100
print("Area under the curve", roc_auc)
print('Confusion Matrix : ', c)

print("  ......      ")

# # Predict on the independent test set
# svc_pred_indep = svc_best.predict(dftrain)

# # Evaluate on the independent test set
# print("Accuracy on independent train set = ", accuracy_score(svc_pred_indep, dftrain) * 100)
# print("Balanced Accuracy = ", balanced_accuracy_score(svc_pred_indep, dftrain) * 100)

# # Calculate additional metrics
# c = confusion_matrix(dftrain, svc_pred_indep)
# sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
# print('Sensitivity : ', sensitivity1)
# specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
# print('Specificity : ', specificity1)
# f1 = f1_score(svc_pred_indep, dftrain, average='macro') * 100
# print("F1-Score : ", f1)
# precision = precision_score(svc_pred_indep, dftrain, average='macro') * 100
# print("Precision : ", precision)
# mcc = matthews_corrcoef(dftrain, svc_pred_indep) * 100
# print("matthews_corrcoef", mcc)
# roc_auc = roc_auc_score(dftrain, svc_pred_indep) * 100
# print("Area under the curve", roc_auc)
# print('Confusion Matrix : ', c)



# Get decision function scores for ROC Curve on independent test set
svc_scores_indep = svc_best.decision_function(X_test)  # Use X_test instead of Y_test

# Calculate ROC Curve for independent test set
fpr_indep, tpr_indep, _ = roc_curve(Y_test, svc_scores_indep)  # Use Y_test for true labels

# Calculate AUC for the ROC Curve
roc_auc_indep = auc(fpr_indep, tpr_indep)

# Plot ROC Curve for independent test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_indep, tpr_indep, color='darkorange', lw=2, label='ROC curve (SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Independent Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score

rfc = RandomForestClassifier()
rfc.fit(X_train, Y_train)
rfc_pred = rfc.predict(X_test)
print("Accuracy on testing = ", accuracy_score(rfc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(rfc_pred, Y_test) * 100)

# Calculate additional metrics
c = confusion_matrix(Y_test, rfc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)
f1 = f1_score(rfc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(rfc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, rfc_pred) * 100
print("matthews_corrcoef", mcc)
roc_auc = roc_auc_score(Y_test, rfc_pred) * 100
print("Area under the curve", roc_auc)
print('Confusion Matrix : ', c)

print("  ......      /")

# rfc_pred = rfc.predict(X_indep_test)
# print("Accuracy on indep testing set = ", accuracy_score(rfc_pred, Y_indep_test) * 100)
# print("Balanced Accuracy = ", balanced_accuracy_score(rfc_pred, Y_indep_test) * 100)

# # Calculate additional metrics
# c = confusion_matrix(Y_indep_test, rfc_pred)
# sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
# print('Sensitivity : ', sensitivity1)
# specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
# print('Specificity : ', specificity1)
# f1 = f1_score(rfc_pred, Y_indep_test, average='macro') * 100
# print("F1-Score : ", f1)
# precision = precision_score(rfc_pred, Y_indep_test, average='macro') * 100
# print("Precision : ", precision)
# mcc = matthews_corrcoef(Y_indep_test, rfc_pred) * 100
# print("matthews_corrcoef", mcc)
# roc_auc = roc_auc_score(Y_indep_test, rfc_pred) * 100
# print("Area under the curve", roc_auc)
# print('Confusion Matrix : ', c)

# Calculate probabilities for ROC Curve on test set
rfc_probs_test = rfc.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
fpr_test, tpr_test, _ = roc_curve(Y_test, rfc_probs_test)
roc_auc_test = roc_auc_score(Y_test, rfc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label='ROC curve (RF area = %0.3f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score

# Initialize and train the Extra Trees Classifier
etc = ExtraTreesClassifier()
etc.fit(X_train, Y_train)
# Make predictions on the test set
etc_pred = etc.predict(X_test)

# Calculate and print metrics
print("Accuracy on testing = ", accuracy_score(etc_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(etc_pred, Y_test) * 100)

# Calculate confusion matrix
c = confusion_matrix(Y_test, etc_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)

# Additional metrics
f1 = f1_score(etc_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(etc_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, etc_pred) * 100
print("Matthews Correlation Coefficient: ", mcc)
roc_auc = roc_auc_score(Y_test, etc_pred) * 100
print("Area Under the Curve: ", roc_auc)
print('Confusion Matrix: ', c)

# Calculate probabilities for ROC Curve on test set
etc_probs_test = etc.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
etc_fpr_test, etc_tpr_test, _ = roc_curve(Y_test, etc_probs_test)
etc_roc_auc_test = roc_auc_score(Y_test, etc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(etc_fpr_test, etc_tpr_test, color='darkorange', lw=2, label='Extra Tree ROC curve (area = %0.3f)' % etc_roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score, roc_curve, auc
import matplotlib.pyplot as plt

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Support Vector Machine": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Extra Trees": ExtraTreesClassifier(random_state=42)
}

# Initialize plot for ROC curves
plt.figure(figsize=(10, 8))

# Iterate through models, train, predict, and evaluate
results = {}
for name, model in models.items():
    print(f"Training and evaluating {name}...")
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    y_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Metrics
    accuracy = accuracy_score(Y_test, y_pred) * 100
    balanced_acc = balanced_accuracy_score(Y_test, y_pred) * 100
    f1 = f1_score(Y_test, y_pred, average='macro') * 100
    precision = precision_score(Y_test, y_pred, average='macro') * 100
    mcc = matthews_corrcoef(Y_test, y_pred) * 100
    roc_auc = roc_auc_score(Y_test, y_probs) * 100 if y_probs is not None else None

    results[name] = {
        "Accuracy": accuracy,
        "Balanced Accuracy": balanced_acc,
        "F1-Score": f1,
        "Precision": precision,
        "MCC": mcc,
        "ROC AUC": roc_auc
    }

    # Print results
    print(f"{name} - Accuracy: {accuracy:.3f}%, Balanced Accuracy: {balanced_acc:.3f}%, F1-Score: {f1:.2f}%, Precision: {precision:.2f}%, MCC: {mcc:.2f}%, ROC AUC: {roc_auc if roc_auc is not None else 'N/A'}")

    # Plot ROC Curve if applicable
    if y_probs is not None:
        fpr, tpr, _ = roc_curve(Y_test, y_probs)
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

# Finalize the ROC curve plot
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for All Models')
plt.legend(loc="lower right")
plt.show()



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Early Stopping and Learning Rate Reduction Callbacks
earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

# Define CNN Model
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the Model
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
cnn_model.summary()

cnn_model.fit(X_train, Y_train, validation_split=0.2,batch_size=32, epochs=100, verbose=1)

# Evaluate on Test Set
predictions = cnn_model.predict(X_test).flatten()
y_pred = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred)
ba = balanced_accuracy_score(Y_test, y_pred)
cm = confusion_matrix(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
mcc = matthews_corrcoef(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- CNN Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Balanced Accuracy: {ba * 100:.3f}%")
print(f"F1-Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Matthews Correlation Coefficient: {mcc:.3f}")
print(f"Sensitivity (Recall): {recall * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"Confusion Matrix:\n{cm}")

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model=cnn_model.predict(X_test)
# calculate roc curves
cnn_model_probs = probs_cnn_model
cnn_model_probs = cnn_model_probs[:, 0]
cnn_model_auc = roc_auc_score(Y_test, cnn_model_probs)
cnn_model_fpr, cnn_model_tpr, threshold = roc_curve(Y_test, cnn_model_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model_fpr, cnn_model_tpr, color='Grey', lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model_auc)





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Early Stopping and Learning Rate Reduction Callbacks
earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

# Define CNN Model
cnn_model2 = Sequential([
    Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the Model
cnn_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
cnn_model2.summary()

cnn_model2.fit(X_train, Y_train, validation_split=0.2,batch_size=32, epochs=100, verbose=1)

# Evaluate on Test Set
predictions = cnn_model2.predict(X_test).flatten()
y_pred1 = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred1)
ba = balanced_accuracy_score(Y_test, y_pred1)
cm = confusion_matrix(Y_test, y_pred1)
f1 = f1_score(Y_test, y_pred1)
precision = precision_score(Y_test, y_pred1)
mcc = matthews_corrcoef(Y_test, y_pred1)
recall = recall_score(Y_test, y_pred1)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- CNN Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Balanced Accuracy: {ba * 100:.3f}%")
print(f"F1-Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Matthews Correlation Coefficient: {mcc:.3f}")
print(f"Sensitivity (Recall): {recall * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"Confusion Matrix:\n{cm}")

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model2=cnn_model2.predict(X_test)
# calculate roc curves
cnn_model2_probs = probs_cnn_model2
cnn_model2_probs = cnn_model2_probs[:, 0]
cnn_model2_auc = roc_auc_score(Y_test, cnn_model2_probs)
cnn_model2_fpr, cnn_model2_tpr, threshold = roc_curve(Y_test, cnn_model2_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, color='Grey', lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model2_auc)



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, matthews_corrcoef, recall_score

# Define MLP Model
mlp_model = Sequential([
    Dense(128, activation='relu', input_shape=(153,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the Model
mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Summary
mlp_model.summary()

mlp_model.fit( X_train, Y_train, validation_split=0.2, batch_size=32,epochs=100,verbose=1)

# Evaluate on Test Set
predictions = mlp_model.predict(X_test).flatten()
y_pred = (predictions > 0.5).astype(int)

# Calculate Metrics
accuracy = accuracy_score(Y_test, y_pred)
ba = balanced_accuracy_score(Y_test, y_pred)
cm = confusion_matrix(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
mcc = matthews_corrcoef(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])

# Print Metrics
print("\n--- MLP Test Metrics ---")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Balanced Accuracy: {ba * 100:.2f}%")
print(f"F1-Score: {f1 * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Matthews Correlation Coefficient: {mcc:.2f}")
print(f"Sensitivity (Recall): {recall * 100:.2f}%")
print(f"Specificity: {specificity * 100:.2f}%")
print(f"Confusion Matrix:\n{cm}")

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1)
mlp_probs = mlp_model.predict(X_test).flatten()

# Calculate ROC AUC
mlp_auc = roc_auc_score(Y_test, mlp_probs)

# Compute ROC Curve
mlp_fpr, mlp_tpr, thresholds = roc_curve(Y_test, mlp_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(mlp_fpr, mlp_tpr, color='blue', lw=2, label='ROC curve (MLP AUC = %0.3f)' % mlp_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for MLP model: %.3f" % mlp_auc)



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Define the LSTM model
lstm_model = Sequential([
    LSTM(128, activation='tanh', return_sequences=True, input_shape=(153, 1)),
    Dropout(0.2),
    LSTM(64, activation='tanh'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # For binary classification
])

# Compile the model
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
lstm_model.summary()

lstm_model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)

# Make predictions on the test set
lstm_predictions = lstm_model.predict(X_test)
lstm_pred_classes = (lstm_predictions > 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(Y_test, lstm_pred_classes)
precision = precision_score(Y_test, lstm_pred_classes)
recall = recall_score(Y_test, lstm_pred_classes)
f1 = f1_score(Y_test, lstm_pred_classes)
roc_auc = roc_auc_score(Y_test, lstm_predictions)

# Confusion Matrix
cm = confusion_matrix(Y_test, lstm_pred_classes)

# Print evaluation metrics
print("LSTM Model Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1) from the LSTM model
lstm_probs = lstm_model.predict(X_test).flatten()

# Calculate ROC AUC
lstm_auc = roc_auc_score(Y_test, lstm_probs)

# Compute ROC Curve
lstm_fpr, lstm_tpr, thresholds = roc_curve(Y_test, lstm_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(lstm_fpr, lstm_tpr, color='blue', lw=2, label='ROC curve (LSTM AUC = %0.3f)' % lstm_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - LSTM')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for LSTM model: %.3f" % lstm_auc)



# Save the trained model
lstm_model.save('lstm_model.h5')
print("Model saved as 'lstm_model.h5'")

from tensorflow.keras.models import load_model

# Load the saved model
loaded_lstm_model = load_model('lstm_model.h5')
print("Model loaded successfully.")

# Display the model summary to confirm
loaded_lstm_model.summary()

# Reshape test data (if not already reshaped)
X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Make predictions
lstm_predictions = loaded_lstm_model.predict(X_test_lstm)
lstm_pred_classes = (lstm_predictions > 0.5).astype(int)

# Evaluate the loaded model
accuracy = accuracy_score(Y_test, lstm_pred_classes)
print(f"Accuracy of the loaded model: {accuracy:.4f}")



from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Save model to Google Drive
lstm_model.save('/content/drive/My Drive/lstm_model.h5')
print("Model saved to Google Drive.")

# Load the model from Google Drive
loaded_lstm_model = load_model('/content/drive/My Drive/lstm_model.h5')
print("Model loaded from Google Drive.")



from sklearn.metrics import roc_curve, roc_auc_score


plt.figure(figsize=(9,8))

plt.plot(fpr_indep, tpr_indep, lw=2, label='ROC curve (SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot(fpr_test, tpr_test, lw=2, label='ROC curve (RF area = %0.3f)' % roc_auc_test)
plt.plot(etc_fpr_test, etc_tpr_test, lw=2, label='ROC curve (ET area = %0.3f)' % etc_roc_auc_test)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, lw=2, label='ROC curve (CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot(mlp_fpr, mlp_tpr, lw=2, label='ROC curve (MLP AUC = %0.3f)' % mlp_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='ROC curve (LSTM AUC = %0.3f)' % lstm_auc)

plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)
#plt.plot([0, 1], [0, 1],  lw=2, color="r", label="Amino Acid Composition ROC Curve", alpha=0.8)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()



from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get predicted probabilities for all models
# Replace these variables with your model predictions
svc_probs = svc_model.predict_proba(X_test)[:, 1]
rf_probs = rf_model.predict_proba(X_test)[:, 1]
et_probs = et_model.predict_proba(X_test)[:, 1]
cnn_probs = cnn_model.predict(X_test).flatten()
mlp_probs = mlp_model.predict(X_test).flatten()
lstm_probs = lstm_model.predict(X_test).flatten()

# Calculate ROC AUC scores
svc_auc = roc_auc_score(Y_test, svc_probs)
rf_auc = roc_auc_score(Y_test, rf_probs)
et_auc = roc_auc_score(Y_test, et_probs)
cnn_auc = roc_auc_score(Y_test, cnn_probs)
mlp_auc = roc_auc_score(Y_test, mlp_probs)
lstm_auc = roc_auc_score(Y_test, lstm_probs)

# Compute ROC curves
svc_fpr, svm_tpr, _ = roc_curve(Y_test, svc_probs)
rf_fpr, rf_tpr, _ = roc_curve(Y_test, rf_probs)
et_fpr, et_tpr, _ = roc_curve(Y_test, et_probs)
cnn_fpr, cnn_tpr, _ = roc_curve(Y_test, cnn_probs)
mlp_fpr, mlp_tpr, _ = roc_curve(Y_test, mlp_probs)
lstm_fpr, lstm_tpr, _ = roc_curve(Y_test, lstm_probs)

# Plot ROC curves
plt.figure(figsize=(12, 12))

plt.plot(svc_fpr, svc_tpr, lw=2, label='SVM (AUC = %0.3f)' % svc_auc)
plt.plot(rf_fpr, rf_tpr, lw=2, label='Random Forest (AUC = %0.3f)' % rf_auc)
plt.plot(et_fpr, et_tpr, lw=2, label='Extra Trees (AUC = %0.3f)' % et_auc)
plt.plot(cnn_fpr, cnn_tpr, lw=2, label='CNN (AUC = %0.3f)' % cnn_auc)
plt.plot(mlp_fpr, mlp_tpr, lw=2, label='MLP (AUC = %0.3f)' % mlp_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='LSTM (AUC = %0.3f)' % lstm_auc)

# Add chance line
plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)

# Plot customization
plt.xlabel('False Positive Rate -->', fontsize=14)
plt.ylabel('True Positive Rate -->', fontsize=14)
plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)
plt.legend(loc="lower right", fontsize=12, ncol=1)
plt.show()





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Define the RNN model
rnn_model = Sequential([
    SimpleRNN(64, activation='tanh', return_sequences=True, input_shape=(153, 1)),
    Dropout(0.5),
    SimpleRNN(32, activation='tanh'),
    Dropout(0.5),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')  # For binary classification
])

# Compile the model
rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
rnn_model.summary()

rnn_model.fit(X_train, Y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=1)

# Predictions on the test set
rnn_predictions = rnn_model.predict(X_test)
rnn_pred_classes = (rnn_predictions > 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(Y_test, rnn_pred_classes)
precision = precision_score(Y_test, rnn_pred_classes)
recall = recall_score(Y_test, rnn_pred_classes)
f1 = f1_score(Y_test, rnn_pred_classes)
roc_auc = roc_auc_score(Y_test, rnn_predictions)

# Confusion Matrix
cm = confusion_matrix(Y_test, rnn_pred_classes)

# Print evaluation metrics
print("RNN Model Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)









from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Updated CNN model with more layers, regularization, and batch normalization
cnn_model = Sequential([
    Conv1D(64, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)),
    BatchNormalization(),
    Conv1D(128, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling1D(pool_size=3, strides=2, padding='same'),
    Dropout(0.3),
    Conv1D(256, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling1D(pool_size=3, strides=2, padding='same'),
    Dropout(0.3),
    Flatten(),
    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.4),
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Compile the model with Adam optimizer and learning rate scheduling
cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
cnn_model.summary()

# Train the model
history = cnn_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)





"""###  LSTM Module"""

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, Activation, Flatten
from tensorflow.keras.layers import LSTM,GRU

import tensorflow as tf
from keras.layers import BatchNormalization
from keras.layers import Input
# from keras.layers.convolutional import Conv1D, MaxPooling1D,MaxPooling2D
from keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from keras.callbacks import EarlyStopping
from keras.layers import Dense
from matplotlib import pyplot

# Reshape for Conv1D, LSTM, RNN, GRU
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# CNN1D
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

cnn_model.fit(X_train_cnn, Y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)
cnn_accuracy = cnn_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'CNN1D Accuracy: {cnn_accuracy:.4f}')

cnn_accuracy = cnn_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'CNN1D Accuracy: {cnn_accuracy:.4f}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, matthews_corrcoef
# Assuming you have already trained your model and have the test data ready
# Predictions on the test set
Y_pred = cnn_model.predict(X_test_cnn)
Y_pred = np.round(Y_pred).astype(int).flatten()

# Calculate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Calculate precision
precision = precision_score(Y_test, Y_pred)
print(f'Precision: {precision:.4f}')

# Calculate recall (sensitivity)
recall = recall_score(Y_test, Y_pred)
print(f'Recall (Sensitivity): {recall:.4f}')

# Calculate F1 score
f1 = f1_score(Y_test, Y_pred)
print(f'F1 Score: {f1:.4f}')

# Generate confusion matrix
conf_matrix = confusion_matrix(Y_test, Y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Calculate specificity
tn, fp, fn, tp = conf_matrix.ravel()
specificity = tn / (tn + fp)
print(f'Specificity: {specificity:.4f}')

# Calculate MCC
mcc = matthews_corrcoef(Y_test, Y_pred)
print(f'Matthews Correlation Coefficient (MCC): {mcc:.4f}')

# Generate classification report
class_report = classification_report(Y_test, Y_pred)
print('Classification Report:')
print(class_report)

# Assuming you have already trained your model and have the test data ready
# Predictions on the test set (probabilities)
probs_cnn = cnn_model.predict(X_test_cnn)

# Calculate roc curves
cnn_probs = probs_cnn[:, 0]
cnn_auc = roc_auc_score(Y_test, cnn_probs)
cnn_fpr, cnn_tpr, threshold = roc_curve(Y_test, cnn_probs)

# Plotting the ROC AUC curve
plt.figure(figsize=(8, 8))
plt.plot(cnn_fpr, cnn_tpr, color='darkorange', lw=2, label='CNN ROC curve (AUC = %0.3f)' % cnn_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(f'CNN AUC: {cnn_auc:.4f}')



# MLP
mlp_model = Sequential([
    Dense(512, input_dim=X_train.shape[1], activation='relu'),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
mlp_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
mlp_accuracy = mlp_model.evaluate(X_test, Y_test, verbose=0)[1]
print(f'MLP Accuracy: {mlp_accuracy:.4f}')

# Evaluate MLP model
mlp_accuracy = mlp_model.evaluate(X_test, Y_test, verbose=0)[1]
print(f'MLP Accuracy: {mlp_accuracy:.4f}')

# Predictions on the test set
Y_pred_mlp = mlp_model.predict(X_test)
Y_pred_mlp = np.round(Y_pred_mlp).astype(int).flatten()

# Calculate accuracy
accuracy_mlp = accuracy_score(Y_test, Y_pred_mlp)
print(f'MLP Accuracy: {accuracy_mlp:.4f}')

# Calculate precision
precision_mlp = precision_score(Y_test, Y_pred_mlp)
print(f'MLP Precision: {precision_mlp:.4f}')

# Calculate recall (sensitivity)
recall_mlp = recall_score(Y_test, Y_pred_mlp)
print(f'MLP Recall (Sensitivity): {recall_mlp:.4f}')

# Calculate F1 score
f1_mlp = f1_score(Y_test, Y_pred_mlp)
print(f'MLP F1 Score: {f1_mlp:.4f}')

# Generate confusion matrix
conf_matrix_mlp = confusion_matrix(Y_test, Y_pred_mlp)
print('MLP Confusion Matrix:')
print(conf_matrix_mlp)

# Calculate specificity (Sp)
tn_mlp, fp_mlp, fn_mlp, tp_mlp = conf_matrix_mlp.ravel()
specificity_mlp = tn_mlp / (tn_mlp + fp_mlp)
print(f'MLP Specificity (Sp): {specificity_mlp:.4f}')

# Calculate sensitivity (Sn)
sensitivity_mlp = recall_mlp
print(f'MLP Sensitivity (Sn): {sensitivity_mlp:.4f}')

# Calculate MCC
mcc_mlp = matthews_corrcoef(Y_test, Y_pred_mlp)
print(f'MLP Matthews Correlation Coefficient (MCC): {mcc_mlp:.4f}')

# Generate classification report
class_report_mlp = classification_report(Y_test, Y_pred_mlp)
print('MLP Classification Report:')
print(class_report_mlp)

# Plot ROC AUC curve
fpr_mlp, tpr_mlp, _ = roc_curve(Y_test, Y_pred_mlp)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)

plt.figure(figsize=(8, 8))
plt.plot(fpr_mlp, tpr_mlp, color='darkorange', lw=2, label='MLP ROC curve (AUC = %0.3f)' % roc_auc_mlp)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(f'MLP AUC: {roc_auc_mlp:.4f}')



# LSTM
lstm_model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.5),
    LSTM(50),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
lstm_model.fit(X_train_cnn, Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
lstm_accuracy = lstm_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'LSTM Accuracy: {lstm_accuracy:.4f}')

# Evaluate LSTM model
lstm_accuracy = lstm_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'LSTM Accuracy: {lstm_accuracy:.4f}')

# Predictions on the test set
Y_pred_lstm = lstm_model.predict(X_test_cnn)
Y_pred_lstm = np.round(Y_pred_lstm).astype(int).flatten()

# Calculate accuracy
accuracy_lstm = accuracy_score(Y_test, Y_pred_lstm)
print(f'LSTM Accuracy: {accuracy_lstm:.4f}')

# Calculate precision
precision_lstm = precision_score(Y_test, Y_pred_lstm)
print(f'LSTM Precision: {precision_lstm:.4f}')

# Calculate recall (sensitivity)
recall_lstm = recall_score(Y_test, Y_pred_lstm)
print(f'LSTM Recall (Sensitivity): {recall_lstm:.4f}')

# Calculate F1 score
f1_lstm = f1_score(Y_test, Y_pred_lstm)
print(f'LSTM F1 Score: {f1_lstm:.4f}')

# Generate confusion matrix
conf_matrix_lstm = confusion_matrix(Y_test, Y_pred_lstm)
print('LSTM Confusion Matrix:')
print(conf_matrix_lstm)

# Calculate specificity (Sp)
tn_lstm, fp_lstm, fn_lstm, tp_lstm = conf_matrix_lstm.ravel()
specificity_lstm = tn_lstm / (tn_lstm + fp_lstm)
print(f'LSTM Specificity (Sp): {specificity_lstm:.4f}')

# Calculate sensitivity (Sn)
sensitivity_lstm = recall_lstm
print(f'LSTM Sensitivity (Sn): {sensitivity_lstm:.4f}')

# Calculate MCC
mcc_lstm = matthews_corrcoef(Y_test, Y_pred_lstm)
print(f'LSTM Matthews Correlation Coefficient (MCC): {mcc_lstm:.4f}')

# Generate classification report
class_report_lstm = classification_report(Y_test, Y_pred_lstm)
print('LSTM Classification Report:')
print(class_report_lstm)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Get predictions on the test set
Y_pred_lstm = lstm_model.predict(X_test_cnn)

# Calculate false positive rate (FPR), true positive rate (TPR), and thresholds
fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(Y_test, Y_pred_lstm)

# Calculate area under the curve (AUC)
roc_auc_lstm = auc(fpr_lstm, tpr_lstm)

# Plot ROC curve
plt.figure(figsize=(8, 8))
plt.plot(fpr_lstm, tpr_lstm, color='darkorange', lw=2, label=f'LSTM ROC curve (AUC = {roc_auc_lstm:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()



# RNN
rnn_model = Sequential([
    SimpleRNN(50, return_sequences=True, input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.5),
    SimpleRNN(50),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_cnn, Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
rnn_accuracy = rnn_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'RNN Accuracy: {rnn_accuracy:.4f}')

# Evaluate RNN model
rnn_accuracy = rnn_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'RNN Accuracy: {rnn_accuracy:.4f}')

# Predictions on the test set
Y_pred_rnn = rnn_model.predict(X_test_cnn)
Y_pred_rnn = np.round(Y_pred_rnn).astype(int).flatten()

# Calculate accuracy
accuracy_rnn = accuracy_score(Y_test, Y_pred_rnn)
print(f'RNN Accuracy: {accuracy_rnn:.4f}')

# Calculate precision
precision_rnn = precision_score(Y_test, Y_pred_rnn)
print(f'RNN Precision: {precision_rnn:.4f}')

# Calculate recall (sensitivity)
recall_rnn = recall_score(Y_test, Y_pred_rnn)
print(f'RNN Recall (Sensitivity): {recall_rnn:.4f}')

# Calculate F1 score
f1_rnn = f1_score(Y_test, Y_pred_rnn)
print(f'RNN F1 Score: {f1_rnn:.4f}')

# Generate confusion matrix
conf_matrix_rnn = confusion_matrix(Y_test, Y_pred_rnn)
print('RNN Confusion Matrix:')
print(conf_matrix_rnn)

# Calculate specificity (Sp)
tn_rnn, fp_rnn, fn_rnn, tp_rnn = conf_matrix_rnn.ravel()
specificity_rnn = tn_rnn / (tn_rnn + fp_rnn)
print(f'RNN Specificity (Sp): {specificity_rnn:.4f}')

# Calculate sensitivity (Sn)
sensitivity_rnn = recall_rnn
print(f'RNN Sensitivity (Sn): {sensitivity_rnn:.4f}')

# Calculate MCC
mcc_rnn = matthews_corrcoef(Y_test, Y_pred_rnn)
print(f'RNN Matthews Correlation Coefficient (MCC): {mcc_rnn:.4f}')

# Generate classification report
class_report_rnn = classification_report(Y_test, Y_pred_rnn)
print('RNN Classification Report:')
print(class_report_rnn)

# Plot ROC AUC curve
fpr_rnn, tpr_rnn, _ = roc_curve(Y_test, Y_pred_rnn)
roc_auc_rnn = auc(fpr_rnn, tpr_rnn)

plt.figure(figsize=(8, 8))
plt.plot(fpr_rnn, tpr_rnn, color='darkorange', lw=2, label=f'RNN ROC curve (AUC = {roc_auc_rnn:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()



# GRU
gru_model = Sequential([
    GRU(128, return_sequences=True, input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.5),
    GRU(64),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
gru_model.fit(X_train_cnn, Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
gru_accuracy = gru_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'GRU Accuracy: {gru_accuracy:.2f}')

# Evaluate GRU model
gru_accuracy = gru_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'GRU Accuracy: {gru_accuracy:.4f}')

# Predictions on the test set
Y_pred_gru = gru_model.predict(X_test_cnn)
Y_pred_gru = np.round(Y_pred_gru).astype(int).flatten()

# Calculate accuracy
accuracy_gru = accuracy_score(Y_test, Y_pred_gru)
print(f'GRU Accuracy: {accuracy_gru:.4f}')

# Calculate precision
precision_gru = precision_score(Y_test, Y_pred_gru)
print(f'GRU Precision: {precision_gru:.4f}')

# Calculate recall (sensitivity)
recall_gru = recall_score(Y_test, Y_pred_gru)
print(f'GRU Recall (Sensitivity): {recall_gru:.4f}')

# Calculate F1 score
f1_gru = f1_score(Y_test, Y_pred_gru)
print(f'GRU F1 Score: {f1_gru:.4f}')

# Generate confusion matrix
conf_matrix_gru = confusion_matrix(Y_test, Y_pred_gru)
print('GRU Confusion Matrix:')
print(conf_matrix_gru)

# Calculate specificity (Sp)
tn_gru, fp_gru, fn_gru, tp_gru = conf_matrix_gru.ravel()
specificity_gru = tn_gru / (tn_gru + fp_gru)
print(f'GRU Specificity (Sp): {specificity_gru:.4f}')

# Calculate sensitivity (Sn)
sensitivity_gru = recall_gru
print(f'GRU Sensitivity (Sn): {sensitivity_gru:.4f}')

# Calculate MCC
mcc_gru = matthews_corrcoef(Y_test, Y_pred_gru)
print(f'GRU Matthews Correlation Coefficient (MCC): {mcc_gru:.4f}')

# Generate classification report
class_report_gru = classification_report(Y_test, Y_pred_gru)
print('GRU Classification Report:')
print(class_report_gru)

# Plot ROC AUC curve
fpr_gru, tpr_gru, _ = roc_curve(Y_test, Y_pred_gru)
roc_auc_gru = auc(fpr_gru, tpr_gru)

plt.figure(figsize=(8, 8))
plt.plot(fpr_gru, tpr_gru, color='darkorange', lw=2, label=f'GRU ROC curve (AUC = {roc_auc_gru:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()



# BiLSTM
from keras.layers import Bidirectional

bilstm_model = Sequential([
    Bidirectional(LSTM(50, return_sequences=True), input_shape=(X_train_cnn.shape[1], 1)),
    Dropout(0.5),
    Bidirectional(LSTM(50)),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
bilstm_model.fit(X_train_cnn, Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
bilstm_accuracy = bilstm_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'BiLSTM Accuracy: {bilstm_accuracy:.4f}')

# Evaluate BiLSTM model
bilstm_accuracy = bilstm_model.evaluate(X_test_cnn, Y_test, verbose=0)[1]
print(f'BiLSTM Accuracy: {bilstm_accuracy:.4f}')

# Predictions on the test set
Y_pred_bilstm = bilstm_model.predict(X_test_cnn)
Y_pred_bilstm = np.round(Y_pred_bilstm).astype(int).flatten()

# Calculate accuracy
accuracy_bilstm = accuracy_score(Y_test, Y_pred_bilstm)
print(f'BiLSTM Accuracy: {accuracy_bilstm:.4f}')

# Calculate precision
precision_bilstm = precision_score(Y_test, Y_pred_bilstm)
print(f'BiLSTM Precision: {precision_bilstm:.4f}')

# Calculate recall (sensitivity)
recall_bilstm = recall_score(Y_test, Y_pred_bilstm)
print(f'BiLSTM Recall (Sensitivity): {recall_bilstm:.4f}')

# Calculate F1 score
f1_bilstm = f1_score(Y_test, Y_pred_bilstm)
print(f'BiLSTM F1 Score: {f1_bilstm:.4f}')

# Generate confusion matrix
conf_matrix_bilstm = confusion_matrix(Y_test, Y_pred_bilstm)
print('BiLSTM Confusion Matrix:')
print(conf_matrix_bilstm)

# Calculate specificity (Sp)
tn_bilstm, fp_bilstm, fn_bilstm, tp_bilstm = conf_matrix_bilstm.ravel()
specificity_bilstm = tn_bilstm / (tn_bilstm + fp_bilstm)
print(f'BiLSTM Specificity (Sp): {specificity_bilstm:.4f}')

# Calculate sensitivity (Sn)
sensitivity_bilstm = recall_bilstm
print(f'BiLSTM Sensitivity (Sn): {sensitivity_bilstm:.4f}')

# Calculate MCC
mcc_bilstm = matthews_corrcoef(Y_test, Y_pred_bilstm)
print(f'BiLSTM Matthews Correlation Coefficient (MCC): {mcc_bilstm:.4f}')

# Generate classification report
class_report_bilstm = classification_report(Y_test, Y_pred_bilstm)
print('BiLSTM Classification Report:')
print(class_report_bilstm)

# Plot ROC AUC curve
fpr_bilstm, tpr_bilstm, _ = roc_curve(Y_test, Y_pred_bilstm)
roc_auc_bilstm = auc(fpr_bilstm, tpr_bilstm)

plt.figure(figsize=(8, 8))
plt.plot(fpr_bilstm, tpr_bilstm, color='darkorange', lw=2, label=f'BiLSTM ROC curve (AUC = {roc_auc_bilstm:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()





from sklearn.metrics import roc_curve, roc_auc_score

plt.figure(figsize=(9, 10))

plt.plot(fpr_lstm, tpr_lstm, lw=2, label='LSTM (AUC = %0.3f)' % roc_auc_lstm)
plt.plot(fpr_gru, tpr_gru, lw=2, label='GRU (AUC = %0.3f)' % roc_auc_gru)
plt.plot(fpr_rnn, tpr_rnn, lw=2, label='RNN (AUC = %0.3f)' % roc_auc_rnn)
plt.plot(fpr_bilstm, tpr_bilstm, lw=2, label= 'BiLSTM (AUC = %0.3f)' % roc_auc_bilstm  )
plt.plot(fpr_mlp, tpr_mlp, lw=2, label='MLP (AUC = %0.3f)' % roc_auc_mlp)
plt.plot(cnn_fpr, cnn_tpr, lw=2, label='CNN (AUC = %0.3f)' % cnn_auc)
plt.plot(fpr_indep, tpr_indep,  lw=2, label='SVM (AUC AUC = %0.3f)' % roc_auc_indep)
plt.plot(fpr_test, tpr_test, lw=2, label='RF (AUC = %0.3f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)


plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()











from keras.models import Sequential
from keras.layers import Dense

# Define the model
vCNNBiLSTM_model = Sequential()
vCNNBiLSTM_model.add(Dense(512, input_dim=2, activation='relu'))
vCNNBiLSTM_model.add(Dropout(0.2))
vCNNBiLSTM_model.add(Dense(256, activation='relu'))
vCNNBiLSTM_model.add(Dropout(0.2))
vCNNBiLSTM_model.add(Dense(128, activation='relu'))
vCNNBiLSTM_model.add(Dense(1, activation='sigmoid'))

# Compile the model
vCNNBiLSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
vCNNBiLSTM_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)



from keras.models import Sequential
from keras.layers import Dense

# Define the model
vCNNRNN_model = Sequential()
vCNNRNN_model.add(Dense(256, input_dim=2, activation='relu'))
vCNNRNN_model.add(Dropout(0.2))
vCNNRNN_model.add(Dense(64, activation='relu'))
vCNNRNN_model.add(Dropout(0.2))
vCNNRNN_model.add(Dense(32, activation='relu'))
vCNNRNN_model.add(Dense(1, activation='sigmoid'))

# Compile the model
vCNNRNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
vCNNRNN_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)









import tensorflow as tf
from tensorflow import keras
from keras.callbacks import EarlyStopping
from keras.layers import Dense, Dropout, Bidirectional, LSTM

# vCNNBiLSTM_model = Sequential()
# vCNNBiLSTM_model = Sequential()
# vCNNBiLSTM_model.add(Bidirectional(LSTM(64, activation='tanh' ,return_sequences=True), input_shape=(2,1)))
# #Bidirectional_model.add(Dropout(0.2))
# vCNNBiLSTM_model.add(Dense(150, activation='relu'))
# vCNNBiLSTM_model.add(Bidirectional(LSTM(32)))
# #Bidirectional_model.add(Dropout(0.2))
# vCNNBiLSTM_model.add(Dense(1, activation='sigmoid'))

# vCNNBiLSTM_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])

# vCNNBiLSTM_model.fit(X_train,Y_train,epochs=100,batch_size=1,validation_split=0.2)



# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras import layers
# from keras.layers import Input
# from keras.models import Sequential
# from keras.layers import Dense, SimpleRNN, Dropout, Activation

# vCNNRNN_model = Sequential()
# vCNNRNN_model.add(SimpleRNN(units=64, input_shape=(20,1)))
# vCNNRNN_model.add(Dense(units=1, activation='sigmoid'))

# # compile the model
# vCNNRNN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# # fit the model to the data
# vCNNRNN_model.fit(X_train, Y_train, validation_split=0.2, batch_size=32, epochs=20, verbose=1)







"""CNN"""

#CNN
from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout
from keras.callbacks import EarlyStopping

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Create sequential model
earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')


cnn_model = Sequential()
cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(153, 1)))
cnn_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))
cnn_model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))
cnn_model.add(MaxPool1D(pool_size=3, strides=2, padding='same'))
cnn_model.add(Dropout(0.2))
cnn_model.add(Flatten())
cnn_model.add(Dense(256, activation='relu'))
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dense(64, activation='relu'))
cnn_model.add(Dense(1, activation='sigmoid'))

cnn_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])

cnn_model.summary()

# cnn_model.fit(X_train,Y_train,epochs=10,batch_size=1,validation_split=0.2)

cnn_model.fit(X_train,Y_train,epochs=100,batch_size=1,validation_split=0.2)

"""from keras.callbacks import EarlyStopping, ModelCheckpoint

earlystop = EarlyStopping(monitor ='loss', min_delta =0 ,patience = 50)"""

import time
start=time.time()
history= cnn_model.fit(X_train, Y_train, validation_split=0.2, batch_size= 32, epochs= 100, verbose= 1, callbacks= [earlystop],)
stop= time.time()
print(f"Training time: {stop - start}s")

# Plot the loss and accuracy curves for training and validation
fig, ax = plt.subplots(2,1, figsize=(18, 10))
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

predictions= cnn_model.predict(X_train)
from sklearn.metrics import accuracy_score
prd6=np.round(predictions)
print("Train of CNN Accuracy of sequence protein task: ", accuracy_score(Y_train, prd6))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_train, prd6)
cm = confusion_matrix(Y_train, prd6)
f1 = f1_score(Y_train, prd6)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd6)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_train, prd6)
mcc = matthews_corrcoef(Y_train, prd6)
recall = recall_score(Y_train, prd6)
# Print evaluation metrics
print("Train of CNN Measure")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

predictions= cnn_model.predict(X_test)
from sklearn.metrics import accuracy_score
prd6=np.round(predictions)
print("Independence of CNN Accuracy of sequence protein task: ", accuracy_score(Y_test, prd6))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_test, prd6)
cm = confusion_matrix(Y_test, prd6)
f1 = f1_score(Y_test, prd6)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd6)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_test, prd6)
mcc = matthews_corrcoef(Y_test, prd6)
recall = recall_score(Y_test, prd6)
# Print evaluation metrics
print("Independenc of CNN Measure")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn=cnn_model.predict(X_test)
# calculate roc curves
cnn_probs = probs_cnn
cnn_probs = cnn_probs[:, 0]
cnn_auc = roc_auc_score(Y_test, cnn_probs)
cnn_fpr, cnn_tpr, threshold = roc_curve(Y_test, cnn_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_fpr, cnn_tpr, color='brown', lw=2, label='CNN ROC curve (AUC = %0.3f)' % cnn_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_auc)





"""### CNN-RNN"""

from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout

from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, GRU, Flatten , SimpleRNN
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.callbacks import EarlyStopping

from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, TimeDistributed ,GRU, Flatten , SimpleRNN

# define the model
cnn_rnn_model = Sequential()
cnn_rnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(153, 1)))
cnn_rnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
cnn_rnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
cnn_rnn_model.add(MaxPooling1D(pool_size=2))
cnn_rnn_model.add(Dropout(0.2))
cnn_model.add(Flatten())
cnn_rnn_model.add(SimpleRNN(64))
#cnn_rnn_model.add(Bidirectional(LSTM(64, return_sequences=True)))
cnn_rnn_model.add(Dropout(0.2))
cnn_rnn_model.add(Dense(1, activation='sigmoid'))

cnn_rnn_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])

cnn_rnn_model.summary()

cnn_rnn_model.fit(X_train,Y_train,epochs=100,batch_size=32,validation_data=(X_test,Y_test))

import time

start=time.time()

history= cnn_rnn_model.fit(X_train, Y_train, validation_split=0.2, batch_size= 32, epochs= 100, verbose= 1 , callbacks= [earlystop],)

stop= time.time()

print(f"Training time: {stop - start}s")

# Plot the loss and accuracy curves for training and validation
fig, ax = plt.subplots(2,1, figsize=(18, 10))
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

predictions= cnn_rnn_model.predict(X_train)
from sklearn.metrics import accuracy_score
prd7=np.round(predictions)
print("Train of CNN-RNN Accuracy: ", accuracy_score(Y_train, prd7))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_train, prd7)
cm = confusion_matrix(Y_train, prd7)
f1 = f1_score(Y_train, prd7)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd7)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_train, prd7)
mcc = matthews_corrcoef(Y_train, prd7)
recall = recall_score(Y_train, prd7)
# Print evaluation metrics
print("Train of CNN-RNN Measure: ")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

predictions= cnn_rnn_model.predict(X_test)
from sklearn.metrics import accuracy_score
prd7=np.round(predictions)
print("Independence of CNN-RNN Accuracy: ", accuracy_score(Y_test, prd7))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_test, prd7)
cm = confusion_matrix(Y_test, prd7)
f1 = f1_score(Y_test, prd7)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd7)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_test, prd7)
mcc = matthews_corrcoef(Y_test, prd7)
recall = recall_score(Y_test, prd7)
# Print evaluation metrics
print("Independence of CNN-RNN Measure: ")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_rnn=cnn_rnn_model.predict(X_test)
# calculate roc curves
cnn_rnn_probs = probs_cnn_rnn
cnn_rnn_probs = cnn_rnn_probs[:, 0]
cnn_rnn_auc = roc_auc_score(Y_test, cnn_rnn_probs)
cnn_rnn_fpr, cnn_rnn_tpr, threshold = roc_curve(Y_test, cnn_rnn_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_rnn_fpr, cnn_rnn_tpr, color='Grey', lw=2, label='CNN-RNN ROC curve (AUC = %0.3f)' % cnn_rnn_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_rnn_auc)





"""### CNN_BiLSTM"""

from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout
from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, GRU, Flatten , SimpleRNN
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, TimeDistributed ,GRU, Flatten , SimpleRNN

# define the model
cnn_bilstm_model = Sequential()
cnn_bilstm_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(153 , 1)))
cnn_bilstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
cnn_bilstm_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
cnn_bilstm_model.add(MaxPooling1D(pool_size=2))
cnn_bilstm_model.add(Dropout(0.2))
#cnn_bilstm_model.add(Flatten())
cnn_bilstm_model.add(Bidirectional(LSTM(64)))
cnn_bilstm_model.add(Dropout(0.2))
cnn_bilstm_model.add(Dense(1, activation='sigmoid'))

cnn_bilstm_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])

cnn_bilstm_model.summary()

cnn_bilstm_model.fit(X_train,Y_train,epochs=100,batch_size=32,validation_data=(X_test,Y_test))

"""from keras.callbacks import EarlyStopping, ModelCheckpoint

earlystop = EarlyStopping(monitor ='loss', min_delta =0 ,patience = 50)"""

import time

start=time.time()

history= cnn_bilstm_model.fit(X_train, Y_train, validation_split=0.2, batch_size= 32, epochs= 100, verbose= 1, callbacks= [earlystop],)

stop= time.time()

print(f"Training time: {stop - start}s")

predictions= cnn_bilstm_model.predict(X_train)
from sklearn.metrics import accuracy_score
prd8train=np.round(predictions)
print("Train of CNN-BiLSTM Accuracy: ", accuracy_score(Y_train, prd8train))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_train, prd8train)
cm = confusion_matrix(Y_train, prd8train)
f1 = f1_score(Y_train, prd8train)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd8train)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_train, prd8train)
mcc = matthews_corrcoef(Y_train, prd8train)
recall = recall_score(Y_train, prd8train)
# Print evaluation metrics
print("Train of CNN-BiLSTM Measure  ")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

predictions= cnn_bilstm_model.predict(X_test)
from sklearn.metrics import accuracy_score
prd8train=np.round(predictions)
print("Independenc of CNN-BiLSTM Accuracy: ", accuracy_score(Y_test, prd8train))

# Calculate evaluation metrics
ba = balanced_accuracy_score(Y_test, prd8train)
cm = confusion_matrix(Y_test, prd8train)
f1 = f1_score(Y_test, prd8train)
spec = cm[0,0]/(cm[0,0]+cm[0,1])
#fpr, tpr, _ = roc_curve(Y_test, prd8train)
#roc_auc = auc(fpr, tpr)
precision = precision_score(Y_test, prd8train)
mcc = matthews_corrcoef(Y_test, prd8train)
recall = recall_score(Y_test, prd8train)
# Print evaluation metrics
print("Independence of CNN-BiLSTM Measure  ")
print("Balance Accuracy: ", ba)
print("Confusion Matrix: ", cm)
print("F1-Score: ", f1)
print("Specificity: ", spec)
#print("Area Under the Curve: ", roc_auc)
print("Precision: ", precision)
print("MCC: ", mcc)
print("Sensitivity: ", recall)
print("Recall: ", recall)

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_bilstm=cnn_bilstm_model.predict(X_test)
# calculate roc curves
cnn_bilstm_probs = probs_cnn_bilstm
cnn_bilstm_probs = cnn_bilstm_probs[:, 0]
cnn_bilstm_auc = roc_auc_score(Y_test, cnn_bilstm_probs)
cnn_bilstm_fpr, cnn_bilstm_tpr, threshold = roc_curve(Y_test, cnn_bilstm_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_bilstm_fpr, cnn_bilstm_tpr, color='Purple', lw=2, label='CNN-BiLSTM ROC curve (AUC = %0.3f)' % cnn_bilstm_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_bilstm_auc)







from sklearn.metrics import roc_curve, roc_auc_score


plt.figure(figsize=(12,12))

#plt.plot(lstm_fpr, lstm_tpr, lw=2, label='FCN (auc = %0.4f)' % lstm_auc)
#plt.plot(rnn_fpr, rnn_tpr, lw=2, label='RNN (auc = %0.4f)' % rnn_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='LSTM(AUC = %0.3f)' % lstm_auc)
plt.plot(gru_fpr, gru_tpr,  lw=2, label='GRU (AUC = %0.3f)' % gru_auc)
plt.plot(rnn_fpr, rnn_tpr,  lw=2, label='RNN (AUC = %0.3f)' % rnn_auc)
plt.plot(bilstm_fpr, bilstm_tpr,  lw=2, label='BiLSTM (AUC = %0.3f)' % bilstm_auc)
plt.plot(mlp_fpr, mlp_tpr,  lw=2, label='MLP (AUC = %0.3f)' % mlp_auc)
plt.plot(cnn_fpr, cnn_tpr,  lw=2, label='CNN (AUC = %0.3f)' % cnn_auc)
#plt.plot(ann_fpr, ann_tpr,  lw=2, label='ANN with batch Nomr (AUC = %0.2f)' % ann_auc)
plt.plot(cnn_rnn_fpr, cnn_rnn_tpr,  lw=2, label='CNN-RNN (AUC = %0.3f)' % cnn_rnn_auc)
plt.plot(cnn_bilstm_fpr, cnn_bilstm_tpr, lw=2, label='CNN-BiLSTM (AUC = %0.3f)' % cnn_bilstm_auc)

plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)
#plt.plot([0, 1], [0, 1],  lw=2, color="r", label="Amino Acid Composition ROC Curve", alpha=0.8)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()



















# Generate the TSNE visualization
## All Dataset base of visulization without model apply.
y=Y
X_tsne = TSNE(n_components=2).fit_transform(X)
y_tsne = y
fig, ax = plt.subplots(1, 3, figsize=(15, 5))
datasets = [(make_moons(noise=0.3, random_state=0), 'Moon Dataset'),
            (make_circles(noise=0.2, factor=0.5, random_state=1), 'Circle Dataset'),
            (make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1,
                                  n_clusters_per_class=1), 'Linearly Separable Dataset')]

for i, dataset in enumerate(datasets):
    ax[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_tsne, cmap='Set1', alpha=0.3)
    ax[i].set_title(dataset[1])
    ax[i].set_xticks([])
    ax[i].set_yticks([])

plt.show()





"""### *Vsiulization Diagram without PCA apply module Digarm **






"""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs, make_checkerboard
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,
                              ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier)
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

X, Y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)

datasets = [make_moons(noise=0.3, random_state=0)
            ,make_circles(noise=0.2, factor=0.5, random_state=1)
            ,make_blobs()
           ]

names = ["GRU","LSTM","RNN","MLP","CNN","BiLSTM-CNN" , "RNN-CNN"]
# Creating a Python List with our three Tree classifiers
treeclassifiers = [

    GRU_model,
    lstm_model,
    RNN_model,
    vMLP1_model,
     vCNN_model,
     vCNNBiLSTM_model,
     vCNNRNN_model

]



#     DecisionTreeClassifier(max_depth=5),
#     RandomForestClassifier(max_depth=5, n_estimators=20, max_features=1),
#     ExtraTreesClassifier()]

figure = plt.figure(figsize=(12, 10))
h = 0.2
i = 1
y=Y
# iterate over datasets
for ds in datasets:
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.jet
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(treeclassifiers) + 1, i)
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.7)
    # and testing points
    #ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, treeclassifiers):
        ax = plt.subplot(len(datasets), len(treeclassifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = np.mean(clf.evaluate(X_test, y_test))

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, m_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
         Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
         Z = (Z > 0.5).astype(int)

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=plt.cm.jet, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.6, linewidths=0.6, edgecolors="white")
        # and testing points
        #ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   #alpha=0.6)
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
        size=15, horizontalalignment='right')
        i += 1
        figure.subplots_adjust(left=.02, right=.98)
plt.show()









"""## *PCA use with Visilization Diagram*"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import re
import os
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix
from statistics import mean
import math
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import matthews_corrcoef
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import seaborn as sns
from sklearn.manifold import TSNE
from __future__ import print_function
import time
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns



pca_50 = PCA(n_components=2)
pca_result_50 = pca_50.fit_transform(X)
print('Cumulative explained variation for 2 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))

time_start = time.time()
tsne = TSNE(n_components=2, verbose=0, perplexity=50, n_iter=300)
tsne_pca_results = tsne.fit_transform(pca_result_50)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))

# visualise again and highlight actual classes of data

target_ids = range(len(Y))

plt.figure(figsize=(10, 8))
colours = ['purple','orange' ]
label = ['Thermopholic Protein','NON-Thermopholic Protein' ]
for i, c, label in zip(target_ids, colours, label):
    plt.scatter(tsne_pca_results[Y == i, 0], tsne_pca_results[Y == i, 1], c=c, label=label, alpha=0.9, linewidths = 2 )
    pass
 # Put the result into a color plot


plt.legend()
plt.show()



# visualise again and highlight actual classes of data

target_ids = range(len(Y))

plt.figure(figsize=(10, 8))
colours = ['purple','orange' ]
label = ['Thermopholic Protein','NON-Thermopholic Protein' ]
for i, c, label in zip(target_ids, colours, label):
    plt.scatter(tsne_pca_results[Y == i, 0], tsne_pca_results[Y == i, 1], c=c, label=label, alpha=0.9, linewidths = 2 )
    pass
 # Put the result into a color plot


plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
h=0.2
names = [
    "GRU","LSTM","RNN","MLP" ,"CNN","BiLSTM-CNN","RNN-CNN"
]

classifiers = [
     GRU_model,
    lstm_model,
    RNN_model,
    vMLP1_model,
     vCNN_model,
     vCNNBiLSTM_model,
     vCNNRNN_model
]

X, y =  tsne_pca_results,y
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

datasets = [
    make_moons(noise=0.3, random_state=0),
    make_circles(noise=0.2, factor=0.5, random_state=1),
    linearly_separable,
]

figure = plt.figure(figsize=(27, 9))
i = 1
y=Y
# iterate over datasets
for ds_cnt, ds in enumerate(datasets):
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(["#FF0000", "#0000FF"])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.evaluate(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            # Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])[:, 1]
              Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
              Z = (Z > 0.5).astype(int)

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.5)
# Plot the training points
        ax.scatter(
            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k"
        )

         # Plot the testing points
        ax.scatter(
            X_test[:, 0],
            X_test[:, 1],
            c=y_test,
            cmap=cm_bright,
            edgecolors="k",
            alpha=0.6,
        )
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        if ds_cnt == 0:
            ax.set_title(name)
        ax.text(
        xx.max() - 0.3,
        yy.min() + 0.3,
        (("%.2f" % score[0]).lstrip("0")),
        size=15,
        horizontalalignment="right",
)
        i += 1

plt.tight_layout()
plt.show()



sns.scatterplot(
    x=tsne_pca_results[:, 0], y=tsne_pca_results[:, 1],
    hue=Y,
    data=X,
    legend="full",
    alpha=0.9
).set(title="Feature Space Visualization of Raw Sequences")







