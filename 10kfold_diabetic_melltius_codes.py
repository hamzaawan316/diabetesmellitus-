# -*- coding: utf-8 -*-
"""10kfold_Diabetic_Melltius_Codes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EyEt6OS0Kb8RI0dJZPrmgCoDTsZyRjj
"""

from google.colab import drive
drive.mount('/content/gdrive')
import os
os.chdir('/content/gdrive/MyDrive')
from google.colab import drive
drive.mount('/content/drive')

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout
from keras.optimizers import SGD
from keras.initializers import random_uniform

dftrain = pd.read_csv('/content/DiabeticPositive1.csv')
#df.head()

##Label and ID represent dataset

Y=dftrain['class']
X=dftrain.drop(['class'],axis=1)

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')

# Option 1: Drop rows with NaN values
X = X.dropna()

# Option 2: Fill NaN values with a default value (e.g., 0)
# X = X.fillna(0)

# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))

# libaray For differnt measure
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.metrics import precision_score, matthews_corrcoef, recall_score, roc_auc_score
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, matthews_corrcoef, recall_score, roc_curve, auc
from sklearn.model_selection import train_test_split
##Split Data
from sklearn.metrics import accuracy_score
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten
from sklearn.manifold import TSNE
from sklearn.datasets import make_moons, make_circles, make_classification
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
)
import numpy as np

# Initialize Dataset (Replace this with your actual dataset)
from sklearn.datasets import make_classification
X, Y = make_classification(
    n_samples=1000, n_features=20, n_classes=2, random_state=42
)

# Define the parameter grid for GridSearch
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [0.1, 1, 10],
    'kernel': ['rbf']  # Include the RBF kernel as the default
}

# Initialize 10K-Fold Cross-Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Initialize metric accumulators
accuracy_list = []
balanced_acc_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
conf_matrix = np.array([[0, 0], [0, 0]])

# Perform 10K-Fold Cross-Validation with GridSearch
for train_index, test_index in kf.split(X, Y):
    # Split the data
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Create SVM model and GridSearchCV instance
    svc = SVC(probability=True, random_state=42)
    grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=3, scoring='accuracy')  # Inner 3-Fold for GridSearch

    # Perform GridSearch on training data
    grid_search.fit(X_train, Y_train)

    # Best model from GridSearch
    svc_model = grid_search.best_estimator_

    # Predict on the test set
    Y_pred = svc_model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(Y_test, Y_pred)
    balanced_acc = balanced_accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)

    # Sensitivity = Recall
    sensitivity = recall  # Same as recall in binary classification

    # Specificity calculation
    cm = confusion_matrix(Y_test, Y_pred)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Update metric accumulators
    accuracy_list.append(accuracy)
    balanced_acc_list.append(balanced_acc)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    conf_matrix += cm

# Compute average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_balanced_acc = np.mean(balanced_acc_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100

# Print results
print(f"10K-Fold Cross-Validation Results (SVM with GridSearchCV):")
print(f"Average Accuracy: {avg_accuracy:.3f}%")
print(f"Average Balanced Accuracy: {avg_balanced_acc:.3f}%")
print(f"Average F1-Score: {avg_f1:.3f}%")
print(f"Average Precision: {avg_precision:.3f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.3f}%")
print(f"Average Specificity: {avg_specificity:.3f}%")
print(f"Confusion Matrix (Sum over all folds):\n{conf_matrix}")

# Get decision function scores for ROC Curve on independent test set
svc_scores_indep = svc_model.decision_function(X_test)  # Use X_test instead of Y_test

# Calculate ROC Curve for independent test set
fpr_indep, tpr_indep, _ = roc_curve(Y_test, svc_scores_indep)  # Use Y_test for true labels

# Calculate AUC for the ROC Curve
roc_auc_indep = auc(fpr_indep, tpr_indep)

# Plot ROC Curve for independent test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_indep, tpr_indep, color='darkorange', lw=2, label='ROC curve (10k-fold SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Independent Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
import numpy as np

# Initialize Dataset (Replace this with your actual dataset)
from sklearn.datasets import make_classification

X, Y = make_classification(
    n_samples=1000, n_features=20, n_classes=2, random_state=42
)

# Define the parameter grid for GridSearch
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [5, 10, 20],
    "min_samples_split": [2, 5, 10],
}

# Initialize 10K-Fold Cross-Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Initialize metric accumulators
accuracy_list = []
balanced_acc_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
conf_matrix = np.array([[0, 0], [0, 0]])

# Perform 10K-Fold Cross-Validation with GridSearch
for train_index, test_index in kf.split(X, Y):
    # Split the data
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Create Random Forest model and GridSearchCV instance
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring="accuracy")  # Inner 3-Fold for GridSearch

    # Perform GridSearch on training data
    grid_search.fit(X_train, Y_train)

    # Best model from GridSearch
    rf_model = grid_search.best_estimator_

    # Predict on the test set
    Y_pred = rf_model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(Y_test, Y_pred)
    balanced_acc = balanced_accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)

    # Sensitivity = Recall
    sensitivity = recall  # Same as recall in binary classification

    # Specificity calculation
    cm = confusion_matrix(Y_test, Y_pred)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Update metric accumulators
    accuracy_list.append(accuracy)
    balanced_acc_list.append(balanced_acc)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    conf_matrix += cm

# Compute average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_balanced_acc = np.mean(balanced_acc_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100

# Print results
print(f"10K-Fold Cross-Validation Results (Random Forest with GridSearchCV):")
print(f"Average Accuracy: {avg_accuracy:.3f}%")
print(f"Average Balanced Accuracy: {avg_balanced_acc:.3f}%")
print(f"Average F1-Score: {avg_f1:.3f}%")
print(f"Average Precision: {avg_precision:.3f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.3f}%")
print(f"Average Specificity: {avg_specificity:.3f}%")
print(f"Confusion Matrix (Sum over all folds):\n{conf_matrix}")

# Calculate probabilities for ROC Curve on test set
rfc_probs_test = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
fpr_test, tpr_test, _ = roc_curve(Y_test, rfc_probs_test)
roc_auc_test = roc_auc_score(Y_test, rfc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label='ROC curve (10k-fold RF area = %0.3f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
import numpy as np

# Initialize Dataset (Replace this with your actual dataset)
from sklearn.datasets import make_classification

X, Y = make_classification(
    n_samples=1000, n_features=20, n_classes=2, random_state=42
)

# Define the parameter grid for GridSearch
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [5, 10, 20],
    "min_samples_split": [2, 5, 10],
}

# Initialize 10K-Fold Cross-Validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Initialize metric accumulators
accuracy_list = []
balanced_acc_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
conf_matrix = np.array([[0, 0], [0, 0]])

# Perform 10K-Fold Cross-Validation with GridSearch
for train_index, test_index in kf.split(X, Y):
    # Split the data
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Create Extra Trees model and GridSearchCV instance
    etc = ExtraTreesClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=etc, param_grid=param_grid, cv=3, scoring="accuracy")  # Inner 3-Fold for GridSearch

    # Perform GridSearch on training data
    grid_search.fit(X_train, Y_train)

    # Best model from GridSearch
    etc_model = grid_search.best_estimator_

    # Predict on the test set
    Y_pred = etc_model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(Y_test, Y_pred)
    balanced_acc = balanced_accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)

    # Sensitivity = Recall
    sensitivity = recall  # Same as recall in binary classification

    # Specificity calculation
    cm = confusion_matrix(Y_test, Y_pred)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Update metric accumulators
    accuracy_list.append(accuracy)
    balanced_acc_list.append(balanced_acc)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    conf_matrix += cm

# Compute average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_balanced_acc = np.mean(balanced_acc_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100

# Print results
print(f"10K-Fold Cross-Validation Results (Extra Trees with GridSearchCV):")
print(f"Average Accuracy: {avg_accuracy:.3f}%")
print(f"Average Balanced Accuracy: {avg_balanced_acc:.3f}%")
print(f"Average F1-Score: {avg_f1:.3f}%")
print(f"Average Precision: {avg_precision:.3f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.3f}%")
print(f"Average Specificity: {avg_specificity:.3f}%")
print(f"Confusion Matrix (Sum over all folds):\n{conf_matrix}")

# Calculate probabilities for ROC Curve on test set
etc_probs_test = etc_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
etc_fpr_test, etc_tpr_test, _ = roc_curve(Y_test, etc_probs_test)
etc_roc_auc_test = roc_auc_score(Y_test, etc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(etc_fpr_test, etc_tpr_test, color='darkorange', lw=2, label='10k-fold Extra Tree ROC curve (area = %0.3f)' % etc_roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization

# # Generate synthetic dataset (replace with your actual dataset)
# from sklearn.datasets import make_classification
# X, Y = make_classification(
#     n_samples=1000, n_features=100, n_classes=2, n_informative=75, random_state=42
# )

# Reshape X for 1D CNN input (samples, timesteps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))

import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef
)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization

# Generate synthetic dataset (replace with your actual dataset)
from sklearn.datasets import make_classification
X, Y = make_classification(
    n_samples=1000, n_features=100, n_classes=2, n_informative=75, random_state=42
)

# Reshape X for 1D CNN input (samples, timesteps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))

# Initialize 10-fold cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Metrics accumulators
accuracy_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
mcc_list = []
conf_matrix_total = np.zeros((2, 2))

# Perform 10-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X, Y), 1):
    print(f"\nTraining Fold {fold}...")

    # Split data into training and testing sets
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Build CNN1D model
    CNN1D = Sequential()
    CNN1D.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))
    CNN1D.add(BatchNormalization())
    CNN1D.add(MaxPooling1D(pool_size=2))

    CNN1D.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
    CNN1D.add(BatchNormalization())
    CNN1D.add(MaxPooling1D(pool_size=2))

    CNN1D.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
    CNN1D.add(BatchNormalization())
    CNN1D.add(MaxPooling1D(pool_size=2))

    CNN1D.add(Flatten())
    CNN1D.add(Dense(128, activation='relu'))
    CNN1D.add(Dropout(0.5))
    CNN1D.add(Dense(1, activation='sigmoid'))

    # Compile model
    CNN1D.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train model
    CNN1D.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=1)

    # Evaluate model on test data
    Y_pred_prob = CNN1D.predict(X_test)  # Predict probabilities
    Y_pred = (Y_pred_prob > 0.5).astype("int32")  # Convert probabilities to binary predictions

    # Calculate metrics for this fold
    accuracy = accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)  # Sensitivity
    cm = confusion_matrix(Y_test, Y_pred)

    # Specificity calculation
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # MCC calculation
    mcc = matthews_corrcoef(Y_test, Y_pred)

    # Append metrics
    accuracy_list.append(accuracy)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    mcc_list.append(mcc)
    conf_matrix_total += cm

# Calculate average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100
avg_mcc = np.mean(mcc_list) * 100

# Print results
print("\n10K-Fold Cross-Validation Results for CNN1D:")
print(f"Average Accuracy: {avg_accuracy:.2f}%")
print(f"Average F1-Score: {avg_f1:.2f}%")
print(f"Average Precision: {avg_precision:.2f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.2f}%")
print(f"Average Specificity: {avg_specificity:.2f}%")
print(f"Average MCC: {avg_mcc:.2f}%")
print("\nConfusion Matrix (Sum over all folds):")
print(conf_matrix_total)

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model2=CNN1D.predict(X_test)
# calculate roc curves
cnn_model2_probs = probs_cnn_model2
cnn_model2_probs = cnn_model2_probs[:, 0]
cnn_model2_auc = roc_auc_score(Y_test, cnn_model2_probs)
cnn_model2_fpr, cnn_model2_tpr, threshold = roc_curve(Y_test, cnn_model2_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, color='Grey', lw=2, label='ROC curve (10k Fold CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model2_auc)



import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef
)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization



# Metrics accumulators
accuracy_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
mcc_list = []
conf_matrix_total = np.zeros((2, 2))

# Perform k-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X, Y), 1):
    print(f"\nTraining Fold {fold}...")

    # Split data into training and testing sets
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Build MLP model
    MLP = Sequential()
    MLP.add(Dense(128, activation='relu', input_dim=X.shape[1]))
    MLP.add(BatchNormalization())
    MLP.add(Dropout(0.5))

    MLP.add(Dense(64, activation='relu'))
    MLP.add(BatchNormalization())
    MLP.add(Dropout(0.5))

    MLP.add(Dense(1, activation='sigmoid'))

    # Compile model
    MLP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train model
    MLP.fit(X_train, Y_train, epochs=20, batch_size=32, verbose=1)

    # Evaluate model on test data
    Y_pred_prob = MLP.predict(X_test)  # Predict probabilities
    Y_pred = (Y_pred_prob > 0.5).astype("int32")  # Convert probabilities to binary predictions

    # Calculate metrics for this fold
    accuracy = accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)  # Sensitivity
    cm = confusion_matrix(Y_test, Y_pred)

    # Specificity calculation
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # MCC calculation
    mcc = matthews_corrcoef(Y_test, Y_pred)

    # Append metrics
    accuracy_list.append(accuracy)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    mcc_list.append(mcc)
    conf_matrix_total += cm

# Calculate average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100
avg_mcc = np.mean(mcc_list) * 100

# Print results
print("\nK-Fold Cross-Validation Results for MLP:")
print(f"Average Accuracy: {avg_accuracy:.3f}%")
print(f"Average F1-Score: {avg_f1:.3f}%")
print(f"Average Precision: {avg_precision:.3f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.3f}%")
print(f"Average Specificity: {avg_specificity:.3f}%")
print(f"Average MCC: {avg_mcc:.3f}%")
print("\nConfusion Matrix (Sum over all folds):")
print(conf_matrix_total)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1)
mlp_probs = MLP.predict(X_test).flatten()

# Calculate ROC AUC
mlp_auc = roc_auc_score(Y_test, mlp_probs)

# Compute ROC Curve
mlp_fpr, mlp_tpr, thresholds = roc_curve(Y_test, mlp_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(mlp_fpr, mlp_tpr, color='blue', lw=2, label='ROC curve (10k Fold MLP AUC = %0.3f)' % mlp_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for MLP model: %.3f" % mlp_auc)



import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef
)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Embedding


# Metrics accumulators
accuracy_list = []
f1_list = []
precision_list = []
recall_list = []
specificity_list = []
mcc_list = []
conf_matrix_total = np.zeros((2, 2))

# Perform 10-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X, Y), 1):
    print(f"\nTraining Fold {fold}...")

    # Split data into training and testing sets
    X_train, X_test = X[train_index], X[test_index]
    Y_train, Y_test = Y[train_index], Y[test_index]

    # Build LSTM model
    LSTM_model = Sequential()
    LSTM_model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
    LSTM_model.add(BatchNormalization())
    LSTM_model.add(Dropout(0.5))
    LSTM_model.add(Dense(64, activation='relu'))
    LSTM_model.add(BatchNormalization())
    LSTM_model.add(Dropout(0.5))
    LSTM_model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    LSTM_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    LSTM_model.fit(X_train, Y_train, epochs=20, batch_size=32, verbose=1)

    # Predict on the test data
    Y_pred_prob = LSTM_model.predict(X_test)  # Probabilities
    Y_pred = (Y_pred_prob > 0.5).astype("int32")  # Binary predictions

    # Calculate metrics for this fold
    accuracy = accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)  # Sensitivity
    cm = confusion_matrix(Y_test, Y_pred)

    # Specificity calculation
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # MCC calculation
    mcc = matthews_corrcoef(Y_test, Y_pred)

    # Append metrics
    accuracy_list.append(accuracy)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    mcc_list.append(mcc)
    conf_matrix_total += cm

# Calculate average metrics
avg_accuracy = np.mean(accuracy_list) * 100
avg_f1 = np.mean(f1_list) * 100
avg_precision = np.mean(precision_list) * 100
avg_recall = np.mean(recall_list) * 100
avg_specificity = np.mean(specificity_list) * 100
avg_mcc = np.mean(mcc_list) * 100

# Print results
print("\n10-Fold Cross-Validation Results for LSTM:")
print(f"Average Accuracy: {avg_accuracy:.2f}%")
print(f"Average F1-Score: {avg_f1:.2f}%")
print(f"Average Precision: {avg_precision:.2f}%")
print(f"Average Sensitivity (Recall): {avg_recall:.2f}%")
print(f"Average Specificity: {avg_specificity:.2f}%")
print(f"Average MCC: {avg_mcc:.2f}%")
print("\nConfusion Matrix (Sum over all folds):")
print(conf_matrix_total)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1) from the LSTM model
lstm_probs = LSTM_model.predict(X_test).flatten()

# Calculate ROC AUC
lstm_auc = roc_auc_score(Y_test, lstm_probs)

# Compute ROC Curve
lstm_fpr, lstm_tpr, thresholds = roc_curve(Y_test, lstm_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(lstm_fpr, lstm_tpr, color='blue', lw=2, label='ROC curve (10k fold LSTM AUC = %0.3f)' % lstm_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - LSTM')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for LSTM model: %.3f" % lstm_auc)

from sklearn.metrics import roc_curve, roc_auc_score


plt.figure(figsize=(9,8))

plt.plot(fpr_indep, tpr_indep, lw=2, label='ROC curve (10k fold SVM AUC = %0.3f)' % roc_auc_indep)
plt.plot(fpr_test, tpr_test, lw=2, label='ROC curve (10k fold RF area = %0.3f)' % roc_auc_test)
plt.plot(etc_fpr_test, etc_tpr_test, lw=2, label='ROC curve (10k fold ET area = %0.3f)' % etc_roc_auc_test)
plt.plot(cnn_model2_fpr, cnn_model2_tpr, lw=2, label='ROC curve (10k fold CNN 1D AUC = %0.3f)' % cnn_model2_auc)
plt.plot(mlp_fpr, mlp_tpr, lw=2, label='ROC curve (10k fold MLP AUC = %0.3f)' % mlp_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='ROC curve (10k fold LSTM AUC = %0.3f)' % lstm_auc)

plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)
#plt.plot([0, 1], [0, 1],  lw=2, color="r", label="Amino Acid Composition ROC Curve", alpha=0.8)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()

