# -*- coding: utf-8 -*-
"""Visualization decision boundary .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hl38aGqB5OGN5MpJCTq1_3NMtffTpTQM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, Input
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from matplotlib.colors import ListedColormap
from sklearn.inspection import DecisionBoundaryDisplay
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM
from tensorflow.keras.utils import to_categorical
from sklearn.neural_network import MLPClassifier
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Conv1D, Flatten, SimpleRNN, Input

# Load dataset
df = pd.read_csv("/content/DiabeticPositive1.csv")
# Separate features and target
X = df.drop(columns=['class'])
Y = df['class']

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')
# Option 1: Drop rows with NaN values
X = X.dropna()
# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))

# Apply t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# Split dataset (now based on t-SNE transformed features)
X_train, X_test, Y_train, Y_test = train_test_split(X_tsne, Y, test_size=0.2, random_state=42, stratify=Y)









# Define classifiers
classifiers = {
    "SVM": SVC(kernel="rbf", probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    "MLP": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42),
}

# CNN Model
cnn = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    Conv1D(32, kernel_size=2, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])
cnn.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
cnn.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# LSTM Model
lstm = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    LSTM(32, return_sequences=False),
    Dense(1, activation="sigmoid")
])
lstm.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
lstm.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# Add CNN and LSTM to classifiers dictionary
classifiers["CNN"] = cnn
classifiers["LSTM"] = lstm

# Create meshgrid for decision boundaries
h = 0.2
x_min, x_max = X_tsne[:, 0].min() - 0.5, X_tsne[:, 0].max() + 0.5
y_min, y_max = X_tsne[:, 1].min() - 0.5, X_tsne[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Define colors
cm = plt.cm.RdBu
cm_bright = ListedColormap(["#FF0000", "#0000FF"])

# Plot decision boundaries
figure = plt.figure(figsize=(17, 12))
i = 1

# Iterate over classifiers
for name, clf in classifiers.items():
    ax = plt.subplot(2, 3, i)

    # Train classifier & calculate accuracy
    if name in ["CNN", "LSTM"]:
        # Reshape for deep learning models
        X_test_reshaped = X_test.reshape(-1, X_train.shape[1], 1)
        Y_pred = (clf.predict(X_test_reshaped) > 0.5).astype(int).flatten()
        score = np.mean(Y_pred == Y_test)
        Z = (clf.predict(np.c_[xx.ravel(), yy.ravel()].reshape(-1, X_train.shape[1], 1)) > 0.5).astype(int).flatten()
    else:
        clf.fit(X_train, Y_train)
        score = clf.score(X_test, Y_test)
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

    # Reshape and plot decision boundary
    Z = Z.reshape(xx.shape)
    contour = ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)

    # Add legend for decision boundary classes
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="red", markersize=10, label="0"),
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="blue", markersize=10, label="1")
    ]
    ax.legend(handles=handles, loc="upper left")

    # Scatter plot for train/test points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cm_bright, edgecolors="black", s=25, label="Train Data")
    ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, alpha=0.6, edgecolors="black", s=25, label="Test Data")

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(name)

    # Display accuracy score
    ax.text(xx.max() - 0.3, yy.min() + 0.2, f"{score:.2f}".lstrip("0"), size=15, horizontalalignment="right")

    i += 1

figure.subplots_adjust(left=0.02, right=0.98)
plt.show()



# Train SVM model
svm = SVC(kernel='rbf', random_state=42)
svm.fit(X_train_scaled, Y_train)
svm_preds = svm.predict(X_test_scaled)
print("SVM Accuracy:", accuracy_score(Y_test, svm_preds))

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, Y_train)
rf_preds = rf.predict(X_test)
print("RF Accuracy:", accuracy_score(Y_test, rf_preds))

# Train Extra Trees
et = ExtraTreesClassifier(n_estimators=100, random_state=42)
et.fit(X_train, Y_train)
et_preds = et.predict(X_test)
print("ET Accuracy:", accuracy_score(Y_test, et_preds))

# Convert labels for deep learning models
Y_train_categorical = to_categorical(Y_train)
Y_test_categorical = to_categorical(Y_test)
X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# CNN Model
cnn = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(Y_train_categorical.shape[1], activation='sigmoid')
])
cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
cnn.fit(X_train_reshaped, Y_train_categorical, epochs=10, batch_size=32, verbose=1)

# LSTM Model
lstm = Sequential([
    LSTM(50, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    Dense(64, activation='relu'),
    Dense(Y_train_categorical.shape[1], activation='sigmoid')
])
lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
lstm.fit(X_train_reshaped, Y_train_categorical, epochs=10, batch_size=32, verbose=1)

# t-SNE visualization
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=Y, palette="viridis", alpha=0.7)
plt.title("t-SNE Visualization of the Dataset")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.legend(title="Class")
plt.show()

