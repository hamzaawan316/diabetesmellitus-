# -*- coding: utf-8 -*-
"""Self_Consistency_Diabetic_Melltius_Codes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1elHmnwsFWi2G12MumIQlKEMq4dv81hsp
"""

from google.colab import drive
drive.mount('/content/gdrive')
import os
os.chdir('/content/gdrive/MyDrive')
from google.colab import drive
drive.mount('/content/drive')

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout
from keras.optimizers import SGD
from keras.initializers import random_uniform

dftrain = pd.read_csv('/content/DiabeticPositive1.csv')
#df.head()

##Label and ID represent dataset

Y=dftrain['class']
X=dftrain.drop(['class'],axis=1)

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')

# Option 1: Drop rows with NaN values
X = X.dropna()

# Option 2: Fill NaN values with a default value (e.g., 0)
# X = X.fillna(0)

# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))

# libaray For differnt measure
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.metrics import precision_score, matthews_corrcoef, recall_score, roc_auc_score
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, matthews_corrcoef, recall_score, roc_curve, auc
from sklearn.model_selection import train_test_split
##Split Data
from sklearn.metrics import accuracy_score
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten
from sklearn.manifold import TSNE
from sklearn.datasets import make_moons, make_circles, make_classification
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
)
import numpy as np

# Generate a synthetic dataset (replace with your dataset)
X, Y = make_classification(
    n_samples=1000, n_features=20, n_classes=2, n_informative=10, random_state=42
)

# Define the SVM model
svc = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)

# Train the model on the entire dataset
svc.fit(X, Y)

# Predict on the same dataset
Y_pred = svc.predict(X)

# Calculate evaluation metrics
accuracy = accuracy_score(Y, Y_pred)
f1 = f1_score(Y, Y_pred)
precision = precision_score(Y, Y_pred)
recall = recall_score(Y, Y_pred)  # Sensitivity
cm = confusion_matrix(Y, Y_pred)

# Specificity calculation
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

# MCC calculation
mcc = matthews_corrcoef(Y, Y_pred)

# Print results
print("SVM Self-Consistency Validation Results:")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"F1-Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Sensitivity (Recall): {recall * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"MCC: {mcc:.3f}")
print("\nConfusion Matrix:")
print(cm)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get decision function scores for ROC Curve on the same dataset (self-consistency)
svc_scores_self_consistency = svc.decision_function(X)  # Use the entire dataset X

# Calculate ROC Curve
fpr_self_consistency, tpr_self_consistency, _ = roc_curve(Y, svc_scores_self_consistency)  # True labels: Y

# Calculate AUC for the ROC Curve
roc_auc_self_consistency = auc(fpr_self_consistency, tpr_self_consistency)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(
    fpr_self_consistency, tpr_self_consistency,
    color='darkorange', lw=2,
    label='ROC Curve (SVM Self-Consistency AUC = %0.3f)' % roc_auc_self_consistency
)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance Line')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Self Consistency (SVM)')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print(f"AUC for SVM Self-Consistency Validation: {roc_auc_self_consistency:.3f}")



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, roc_curve, auc
)
import numpy as np
import matplotlib.pyplot as plt

# Initialize the Random Forest model
rf = RandomForestClassifier(n_estimators=2, random_state=42)

# Train and predict on the same dataset (self-consistency)
rf.fit(X, Y)  # Training on the entire dataset
Y_pred = rf.predict(X)  # Predicting on the same dataset

# Calculate metrics
accuracy = accuracy_score(Y, Y_pred)
balanced_acc = balanced_accuracy_score(Y, Y_pred)
f1 = f1_score(Y, Y_pred, average='macro')
precision = precision_score(Y, Y_pred, average='macro')
recall = recall_score(Y, Y_pred, average='macro')
mcc = matthews_corrcoef(Y, Y_pred)

# Confusion Matrix and Specificity
conf_matrix = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = conf_matrix.ravel()
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

# ROC Curve and AUC
rf_probs = rf.predict_proba(X)[:, 1]  # Probabilities for the positive class
rf_fpr, rf_tpr, _ = roc_curve(Y, rf_probs)
roc_auc = auc(rf_fpr, rf_tpr)

# Print metrics
print("Random Forest Self-Consistency Results:")
print(f"Accuracy: {accuracy * 100:.3f}%")
print(f"Balanced Accuracy: {balanced_acc * 100:.3f}%")
print(f"F1 Score: {f1 * 100:.3f}%")
print(f"Precision: {precision * 100:.3f}%")
print(f"Sensitivity (Recall): {sensitivity * 100:.3f}%")
print(f"Specificity: {specificity * 100:.3f}%")
print(f"MCC: {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}")
print("Confusion Matrix:")
print(conf_matrix)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(rf_fpr, rf_tpr, color='darkorange', lw=2, label='RF Self-Consistency ROC Curve (AUC = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance Line')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest (Self Consistency)')
plt.legend(loc="lower right")
plt.show()

# Calculate probabilities for ROC Curve on test set
rfc_probs_test = rf.predict_proba(X)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
fpr_test, tpr_test, _ = roc_curve(Y, rfc_probs_test)
roc_auc_test = roc_auc_score(Y, rfc_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label='ROC curve (10k-fold RF area = %0.3f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, roc_auc_score, precision_score
# Initialize and train the Extra Trees Classifier
et = ExtraTreesClassifier()
et.fit(X_train, Y_train)
# Make predictions on the test set
et_pred = et.predict(X_test)

# Calculate and print metrics
print("Accuracy on testing = ", accuracy_score(et_pred, Y_test) * 100)
print("Balanced Accuracy = ", balanced_accuracy_score(et_pred, Y_test) * 100)

# Calculate confusion matrix
c = confusion_matrix(Y_test, et_pred)
sensitivity1 = (c[0, 0] / (c[0, 0] + c[0, 1]) * 100)
print('Sensitivity : ', sensitivity1)
specificity1 = (c[1, 1] / (c[1, 0] + c[1, 1]) * 100)
print('Specificity : ', specificity1)

# Additional metrics
f1 = f1_score(et_pred, Y_test, average='macro') * 100
print("F1-Score : ", f1)
precision = precision_score(et_pred, Y_test, average='macro') * 100
print("Precision : ", precision)
mcc = matthews_corrcoef(Y_test, et_pred) * 100
print("Matthews Correlation Coefficient: ", mcc)
roc_auc = roc_auc_score(Y_test, et_pred) * 100
print("Area Under the Curve: ", roc_auc)
print('Confusion Matrix: ', c)

# Calculate probabilities for ROC Curve on test set
et_probs_test = et.predict_proba(X_test)[:, 1]  # Probabilities of positive class (class 1)

# Calculate ROC Curve for test set
et_fpr_test, et_tpr_test, _ = roc_curve(Y_test, et_probs_test)
et_roc_auc_test = roc_auc_score(Y_test, et_probs_test)

# Plot ROC Curve for test set
plt.figure(figsize=(8, 6))
plt.plot(et_fpr_test, et_tpr_test, color='darkorange', lw=2, label='Self-Consistencey ET ROC curve (area = %0.3f)' % et_roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')
plt.legend(loc="lower right")
plt.show()







from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Initialize Dataset (Add Noise to Simulate Real-World Conditions)
from sklearn.datasets import make_classification
X, Y = make_classification(
    n_samples=1000, n_features=20, n_classes=2, flip_y=0.05, random_state=42
)  # `flip_y` adds noise

# Step 2: Initialize the Extra Trees Classifier
et_classifier = ExtraTreesClassifier(random_state=42)

# Step 3: Train the model on the entire dataset (Self-Consistency)
et_classifier.fit(X, Y)

# Step 4: Predict on the same dataset
Y_pred = et_classifier.predict(X)

# Step 5: Calculate probabilities for ROC Curve
et_probs = et_classifier.predict_proba(X)[:, 1]  # Probabilities for the positive class (class 1)

# Step 6: Compute Metrics
accuracy = accuracy_score(Y, Y_pred) * 100
balanced_acc = balanced_accuracy_score(Y, Y_pred) * 100
f1 = f1_score(Y, Y_pred) * 100
precision = precision_score(Y, Y_pred) * 100
recall = recall_score(Y, Y_pred) * 100  # Sensitivity
cm = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0.0
mcc = matthews_corrcoef(Y, Y_pred)
roc_auc = roc_auc_score(Y, et_probs) * 100

# Step 7: Print Metrics
print("Extra Trees Classifier Self-Consistency Results:")
print(f"Accuracy: {accuracy:.3f}%")
print(f"Balanced Accuracy: {balanced_acc:.3f}%")
print(f"F1 Score: {f1:.3f}%")
print(f"Precision: {precision:.3f}%")
print(f"Sensitivity (Recall): {recall:.3f}%")
print(f"Specificity: {specificity:.3f}%")
print(f"MCC: {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}")
print("Confusion Matrix:")
print(cm)

# Step 8: Plot ROC Curve
fpr, tpr, _ = roc_curve(Y, et_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='Self-Consistency Extra Trees ROC curve (AUC = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Self-Consistency')
plt.legend(loc="lower right")
plt.show()



import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.datasets import make_classification
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt

# Step 2: Build CNN1D Model
CNN1D_model = Sequential([
    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(153, 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Step 3: Compile the Model
CNN1D_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the Model (Self-Consistency: Train and Test on the Same Data)
history = CNN1D_model.fit(X, Y, epochs=100, batch_size=32, verbose=1)

# Step 5: Predict on the Same Dataset
Y_pred_probs = CNN1D_model.predict(X).ravel()
Y_pred = (Y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Step 6: Compute Metrics
accuracy = accuracy_score(Y, Y_pred) * 100
balanced_acc = balanced_accuracy_score(Y, Y_pred) * 100
f1 = f1_score(Y, Y_pred) * 100
precision = precision_score(Y, Y_pred) * 100
recall = recall_score(Y, Y_pred) * 100  # Sensitivity
cm = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0.0
mcc = matthews_corrcoef(Y, Y_pred)
roc_auc = roc_auc_score(Y, Y_pred_probs) * 100

# Step 7: Print Metrics
print("CNN1D Self-Consistency Results:")
print(f"Accuracy: {accuracy:.3f}%")
print(f"Balanced Accuracy: {balanced_acc:.3f}%")
print(f"F1 Score: {f1:.3f}%")
print(f"Precision: {precision:.3f}%")
print(f"Sensitivity (Recall): {recall:.3f}%")
print(f"Specificity: {specificity:.3f}%")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}%")
print("Confusion Matrix:")
print(cm)

# Step 8: Plot ROC Curve
fpr, tpr, _ = roc_curve(Y, Y_pred_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Self-Consistency')
plt.legend(loc="lower right")
plt.show()



import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.datasets import make_classification
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt

# Step 1: Initialize Dataset (Replace with your actual dataset)
X, Y = make_classification(
    n_samples=1000, n_features=100, n_classes=2, n_informative=75,
    flip_y=0.05, random_state=42
)

# Reshape the dataset for CNN1D (samples, timesteps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)  # Adding a channel dimension for 1D Conv

# Step 2: Build CNN1D Model
CNN1D = Sequential([
    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Step 3: Compile the Model
CNN1D.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the Model (Self-Consistency: Train and Test on the Same Data)
history = CNN1D.fit(X, Y, epochs=10, batch_size=32, verbose=1)

# Step 5: Predict on the Same Dataset
Y_pred_probs = CNN1D.predict(X).ravel()
Y_pred = (Y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Step 6: Compute Metrics
accuracy = accuracy_score(Y, Y_pred) * 100
balanced_acc = balanced_accuracy_score(Y, Y_pred) * 100
f1 = f1_score(Y, Y_pred) * 100
precision = precision_score(Y, Y_pred) * 100
recall = recall_score(Y, Y_pred) * 100  # Sensitivity
cm = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0.0
mcc = matthews_corrcoef(Y, Y_pred)
roc_auc = roc_auc_score(Y, Y_pred_probs) * 100

# Step 7: Print Metrics
print("CNN1D Self-Consistency Results:")
print(f"Accuracy: {accuracy:.3f}%")
print(f"Balanced Accuracy: {balanced_acc:.3f}%")
print(f"F1 Score: {f1:.3f}%")
print(f"Precision: {precision:.3f}%")
print(f"Sensitivity (Recall): {recall:.3f}%")
print(f"Specificity: {specificity:.3f}%")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}%")
print("Confusion Matrix:")
print(cm)

#predict_classes=np.argmax(predict_prob,axis=1)
from sklearn.metrics import roc_curve, roc_auc_score
probs_cnn_model=CNN1D.predict(X)
# calculate roc curves
cnn_model_probs = probs_cnn_model
cnn_model_probs = cnn_model_probs[:, 0]
cnn_model_auc = roc_auc_score(Y, cnn_model_probs)
cnn_model_fpr, cnn_model_tpr, threshold = roc_curve(Y, cnn_model_probs)
plt.figure(figsize=(8,8))

#plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
plt.plot(cnn_model_fpr, cnn_model_tpr, color='Grey', lw=2, label='ROC curve (CNN-1D Self-Consistency AUC = %0.3f)' % cnn_model_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, label="Chance" ,linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#plt.plot(lstm_fpr, lstm_tpr, marker='.', label='LSTM' % lstm_auc)
print(cnn_model_auc)



import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, roc_curve
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Step 1: Initialize Dataset (Replace with your actual dataset)
X, Y = make_classification(
    n_samples=1000, n_features=100, n_classes=2, n_informative=75,
    flip_y=0.03, random_state=42
)

# Step 2: Build MLP Model
MLP = Sequential([
    Dense(128, activation='relu', input_shape=(X.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Step 3: Compile the Model
MLP.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the Model (Self-Consistency: Train and Test on the Same Data)
history = MLP.fit(X, Y, epochs=50, batch_size=32, verbose=1)

# Step 5: Predict on the Same Dataset
Y_pred_probs = MLP.predict(X).ravel()
Y_pred = (Y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Step 6: Compute Metrics
accuracy = accuracy_score(Y, Y_pred) * 100
balanced_acc = balanced_accuracy_score(Y, Y_pred) * 100
f1 = f1_score(Y, Y_pred) * 100
precision = precision_score(Y, Y_pred) * 100
recall = recall_score(Y, Y_pred) * 100  # Sensitivity
cm = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0.0
mcc = matthews_corrcoef(Y, Y_pred)
roc_auc = roc_auc_score(Y, Y_pred_probs) * 100

# Step 7: Print Metrics
print("MLP Self-Consistency Results:")
print(f"Accuracy: {accuracy:.3f}%")
print(f"Balanced Accuracy: {balanced_acc:.3f}%")
print(f"F1 Score: {f1:.3f}%")
print(f"Precision: {precision:.3f}%")
print(f"Sensitivity (Recall): {recall:.3f}%")
print(f"Specificity: {specificity:.3f}%")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}%")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1)
mlp_probs = MLP.predict(X).flatten()

# Calculate ROC AUC
mlp_auc = roc_auc_score(Y, mlp_probs)

# Compute ROC Curve
mlp_fpr, mlp_tpr, thresholds = roc_curve(Y, mlp_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(mlp_fpr, mlp_tpr, color='blue', lw=2, label='ROC curve (MLP Self-Consistency AUC = %0.3f)' % mlp_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - MLP')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for MLP model: %.3f" % mlp_auc)



import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, confusion_matrix,
    f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, roc_curve
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Step 1: Generate a Synthetic Dataset (Replace with your actual dataset)
X, Y = make_classification(
    n_samples=1000, n_features=50, n_classes=2, n_informative=40, flip_y=0.03, random_state=42
)

# Reshape the data for LSTM input (samples, timesteps, features)
X = X.reshape(X.shape[0], 1, X.shape[1])

# Step 2: Build LSTM Model
Lstm_model = Sequential([
    LSTM(64, activation='tanh', input_shape=(X.shape[1], X.shape[2])),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Step 3: Compile the Model
Lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the Model (Self-Consistency: Train and Test on the Same Data)
history = Lstm_model.fit(X, Y, epochs=15, batch_size=32, verbose=1)

# Step 5: Predict on the Same Dataset
Y_pred_probs = Lstm_model.predict(X).ravel()
Y_pred = (Y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Step 6: Compute Metrics
accuracy = accuracy_score(Y, Y_pred) * 100
balanced_acc = balanced_accuracy_score(Y, Y_pred) * 100
f1 = f1_score(Y, Y_pred) * 100
precision = precision_score(Y, Y_pred) * 100
recall = recall_score(Y, Y_pred) * 100  # Sensitivity
cm = confusion_matrix(Y, Y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0.0
mcc = matthews_corrcoef(Y, Y_pred)
roc_auc = roc_auc_score(Y, Y_pred_probs) * 100

# Step 7: Print Metrics
print("LSTM Self-Consistency Results:")
print(f"Accuracy: {accuracy:.3f}%")
print(f"Balanced Accuracy: {balanced_acc:.3f}%")
print(f"F1 Score: {f1:.3f}%")
print(f"Precision: {precision:.3f}%")
print(f"Sensitivity (Recall): {recall:.3f}%")
print(f"Specificity: {specificity:.3f}%")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.3f}")
print(f"AUC: {roc_auc:.3f}%")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the positive class (class 1) from the LSTM model
lstm_probs = Lstm_model.predict(X).flatten()

# Calculate ROC AUC
lstm_auc = roc_auc_score(Y, lstm_probs)

# Compute ROC Curve
lstm_fpr, lstm_tpr, thresholds = roc_curve(Y, lstm_probs)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(lstm_fpr, lstm_tpr, color='blue', lw=2, label='ROC curve (LSTM Self- Consistency AUC = %0.3f)' % lstm_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Chance")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - LSTM')
plt.legend(loc="lower right")
plt.show()

# Print AUC
print("AUC for LSTM model: %.3f" % lstm_auc)



from sklearn.metrics import roc_curve, roc_auc_score
plt.figure(figsize=(10,10))

plt.plot(fpr_self_consistency, tpr_self_consistency,lw=2,label='ROC curve (SVM Self-Consistency AUC = %0.3f' % roc_auc_self_consistency)
plt.plot(rf_fpr, rf_tpr,  lw=2, label='ROC curve (RF Self-Consistency AUC = %0.3f)' % roc_auc)
plt.plot(et_fpr_test, et_tpr_test, lw=2, label='ROC curve (ET Self-Consistency AUC = %0.3f)' % et_roc_auc_test)
plt.plot(cnn_model_fpr, cnn_model_tpr, lw=2, label='ROC curve (CNN-1D Self-Consistency AUC = %0.3f)' % cnn_model_auc)
plt.plot(mlp_fpr, mlp_tpr, lw=2, label='ROC curve (MLP Self-Consistency AUC = %0.3f)' % mlp_auc)
plt.plot(lstm_fpr, lstm_tpr, lw=2, label='ROC curve (LSTM Self-Consistency AUC = %0.3f)' % lstm_auc)

plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.5)
#plt.plot([0, 1], [0, 1],  lw=2, color="r", label="Amino Acid Composition ROC Curve", alpha=0.8)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=15, ncol=1)

plt.show()

